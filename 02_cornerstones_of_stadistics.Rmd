---
title: "02_cornerstones_of_stadistics"
author: "Fran Camacho"
date: "2025-08-06"
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
---

# 02 - Cornerstones of Stadistics


```{r}
#Libraries

#https://tidyverse.r-universe.dev/tidyverse
if (!require(tidyverse)) install.packages('tidyverse', repos = c('https://tidyverse.r-universe.dev', 'https://cloud.r-project.org'))
library(tidyverse)

# Install the packages:
#install.packages(c("gtsummary", "flextable", "ivo.table"))

if (!require(gtsummary)) install.packages('gtsummary', dependencies = T)
library(gtsummary)
if (!require(flextable)) install.packages('flextable', dependencies = T)
library(flextable)
if (!require(ivo.table)) install.packages('ivo.table', dependencies = T)
library(ivo.table)


```

## 3.1 The three cultures

There are three main schools in statistical modelling: 
the frequentist school, 
the Bayesian school, and 
the machine learning school.


## 3.2 Frequencies, proportions, and cross-tables


```{r}
library(palmerpenguins)
?penguins
```


- Frequency tables:
Frequency tables are used to summarise the distribution of a single variable. 

Without pipes:
```{r}
table(penguins$species)
```

With pipes:
```{r}
library(dplyr)
penguins |> select(species) |> table()
```


- Publication-ready tables

The table in the previous section is fine for data exploration but requires some formatting if you wish to put it in a report. 
If instead you want something publication-ready that you can put straight into a report or a presentation, 
I recommend using either the **gtsummary** package or the **ivo.table** package. 

```{r}
# Install the packages:
#install.packages(c("gtsummary", "flextable", "ivo.table"))

if (!require(gtsummary)) install.packages('gtsummary', dependencies = T)

if (!require(flextable)) install.packages('flextable', dependencies = T)
library(flextable)
if (!require(ivo.table)) install.packages('ivo.table', dependencies = T)
library(ivo.table)
```


Without pipes:
```{r}
library(gtsummary)

tbl_summary(penguins[,"species"])
```

With pipes:
```{r}
library(dplyr)
library(gtsummary)

penguins |>
  select(species) |>
  tbl_summary()
```


The **ivo.table** package and the **ivo_table** function offer a similar table constructed in the same way:

Without pipes:
```{r}
library(ivo.table)

ivo_table(penguins[,"species"])
```


With pipes:
```{r}
library(dplyr)
library(ivo.table)

penguins |>
  select(species) |>
  ivo_table()
```

You can modify the settings to change the colours and fonts used, and to show percentages instead of counts:
```{r}
penguins |>
  select(species) |>
  ivo_table(color = "darkred",
            font = "Garamond",
            percent_by = "row")
```

If you prefer, you can also get the table in a long format that is useful when your variable has many different levels:
```{r}
penguins |>
  select(species) |>
  ivo_table(long_table = TRUE)
```

Finally, the table can be exported to Word as follows:
```{r}
library(flextable)

penguins |>
  select(species) |>
  ivo_table() |> 
  save_as_docx(path = "my_table.docx")
```



### Contingency tables

Contingency tables, also known as **cross-tabulations** or **cross-tables**, are used to summarise the relationship between two or more variables.
They consist of a table with rows and columns that represent the different values of the variables, and the entries in the table show the number or proportion of times each combination of values occurs in the data. For example, a contingency table for the variables species and island in the penguins data shows the number of individuals of different species at different islands.


Without pipes:
```{r}
ftable(penguins$island, penguins$species)
```

With pipes:
```{r}
penguins |> select(island, species) |> ftable()
```

Again, we can create a nicely formatted publication-ready table. With ivo_table, we follow the same logic as before:
```{r}
library(ivo.table)

penguins |> select(species, island) |>
            ivo_table()
```

To create a contingency table using tbl_summary, we use the following syntax:

Without pipes:
```{r}
library(gtsummary)

tbl_summary(penguins[,c("species","island")], by = species)
#tbl_summary(penguins[,c("species","island")], by = island)
```


With pipes:
```{r}
library(dplyr)
library(gtsummary)

penguins |> select(species, island) |> tbl_summary(by=species)
```


Contingency tables are great for presenting data, and in many cases we can draw conclusions directly from looking at such a table. 
For instance, in the example above, it is clear that Adelie penguins are the only species present at all three islands 
(or at least, the only species sampled at all three islands). In other cases, the results aren’t as clear-cut. 
That’s when statistical hypothesis testing becomes useful ...


- Three-way and four-way tables

Three-way and four-way tables are contingency tables showing the distribution of three and four variables, respectively. 
They are straightforward to create using ftable or ivo_table.

We simply select the variables we wish to include in the table, and use ftable or ivo_table as in previous examples.
Here are some examples using ivo_table:


```{r}
# A three-way table:
library(ivo.table)

penguins |> select(sex, species, island) |>
            ivo_table()
```


```{r}
# Exclude missing values:
penguins |> select(sex, species, island) |>
            ivo_table(exclude_missing = TRUE)
```

```{r}
# A four-way table:
penguins |> select(sex, species, island, year) |>
            ivo_table()
```


You can’t use gtsummary to construct three-way and four-way tables, but you can use it for a similar type of table, presenting several variables at once, stratified by another variable. For instance, we can show the frequencies of islands and sexes stratified by species, as in the example below. This type of table, showing the distributions of different categorical variables, is often referred to as **Table 1** in scientific papers.

Without pipes:
```{r}
library(gtsummary)

tbl_summary(penguins[,c("species","sex","island")], by = species)
```


With pipes:
```{r}
library(dplyr)
library(gtsummary)

penguins |>
  select(species, sex, island) |>
  tbl_summary(by = species)
```



## 3.3 Hypothesis testing and p-values

https://modernstatisticswithr.com/basicstatistics.html#hypotheses


**Statistical hypothesis testing** is used to determine which of two complementary hypotheses is true. 
In statistics, a hypothesis is a statement about a parameter in a population, such as the population mean value. 
The two hypotheses in a hypothesis testing problem are:

- The **null hypothesis H0**
corresponding to “no effect”, “no difference”, or “no relationship”.

- The **alternative hypothesis H1**
corresponding to “there is an effect”, “there is a difference”, or “there is a relationship”.

example: Comparing two groups. 

To find out whether flipper length differs between male and female Chinstrap penguins, we measure the flipper lengths 
of a number of penguins. If the average length in the female population is μ1 and the average length in the male 
population is μ2, then the population parameter that we are interested in is the difference μ1−μ2
The hypotheses are:

H0 : there is no difference; μ1−μ2=0
H1 : there is a difference; μ1−μ2≠0.

The purpose of hypothesis testing is to determine which of the two hypotheses to believe in.
Hypothesis testing is often compared to legal trials: the null hypothesis is considered to be “innocent until proven guilty”, 
meaning that we won’t reject it unless there is compelling evidence against it. 
We can therefore think of the null hypothesis as a sort of default – we’ll believe in it until 
we have enough evidence to say with some confidence that it in fact isn’t true.


### The lady tasting tea

...

The hypotheses that this experiment was designed to test are:

H0: the lady’s guesses are no better than chance.
H1: the lady’s guesses are better than chance.

The results of the experiment can be presented in a 2×2 contingency table. 
For instance, one possible outcome is that the lady gets both sets of four right, which yields the following table:

	                 Truth: milk first 	Truth: tea first
Guess: milk first     	4 	               0
Guess: tea first 	      0 	               4 

The design of this experiment is such that all row sums and column sums are 4. 
Consequently, if we know the count in the upper left cell we also know the counts in the other cells.
This means that there are 5 possible tables that can result (the count in the upper left cell can be 4, 3, 2, 1, or 0). 
We can compute the probability of obtaining each table under the null hypothesis21, with the following results:

- The count is 4 (4 cups with milk poured first right), with probability 1/70,
- The count is 3 (3 cups with milk poured first right and 1 wrong), with probability 16/70,
- The count is 2 (2 cups with milk poured first right and 2 wrong), with probability 36/70,
- The count is 1 (1 cup with milk poured first right and 3 wrong), with probability 16/70,
- The count is 0 (4 cups with milk poured first wrong), with probability 1/70.

...

Getting 3 cups where the milk poured first right is the second most extreme outcome. 
If H0 is true, this occurs in 16 experiments out of 70. In addition, a more extreme outcome (4 cups right) occurs in 1 experiment out of 70. 
The probability of getting a result that is at least as extreme as this is therefore 16/70+1/70=17/70, and the p-value given this outcome is 17/70 ≈ 0.24.

...

### How low does the p-value have to be?

https://modernstatisticswithr.com/basicstatistics.html#how-low-does-the-p-value-have-to-be

We need some way of determining a cut-off for p-values. Let’s call this cut-off α, the significance level of our test. 
It can be any number between 0 and 1. Once we’ve decided on a value for α, we are ready to make a decision regarding which hypothesis to believe in. 
If the p-value is less than α, we reject H0 in favour of H1 and say the result is statistically significant. 
If the p-value is greater than α, we conclude that there isn’t sufficient evidence against H0, so we’ll believe in it for the time being.

How then shall we choose α? There are two types of errors that we can make in hypothesis testing:

- A type I error: falsely rejecting H0 even though H0 is true (a false positive result).
- A type II error: not rejecting H0 even though H1 is true (a false negative result).

Both of these will depend on what significance level we choose for our p-value. 
If we choose a low α, we require more evidence before we reject H0. 
This lowers the risk of a type I error but increases the risk of a type II error. 

The probability of making a type I error is the easiest to control and often also considered to be the most important of the two.

...

In the real world, there are two types of tests:

- Exact tests, for which the probability of committing a type I error is less than or equal to α
- Approximate tests, for which the probability of committing a type II error is approximately equal to α
(but may be greater than α, perhaps substantially so). How close it is to α depends on a number of factors.
(We’ll discuss these for different tests later in this chapter).


### Fisher’s exact test

https://modernstatisticswithr.com/basicstatistics.html#fishers-exact-test


To use Fisher’s exact test, we first need some data. 

The contingency table describing the outcome of the experiment can be constructed in different ways in R, depending on whether our data is stored in a long data frame or already available in tabulated form. If it is stored in a data frame, where each row shows the outcome of a repetition, we can get a contingency table as follows:

```{r}
# Compute the contingency table from a data frame:
lady_data <- data.frame(Guess = c("Milk first", "Milk first",
                                  "Tea first", "Tea first",
                                  "Milk first", "Tea first",
                                  "Tea first", "Milk first"),
                        Truth = c("Milk first", "Milk first",
                                  "Tea first", "Tea first",
                                  "Milk first", "Tea first",
                                  "Tea first", "Milk first"))

lady_data |> ftable() -> lady_results1

lady_results1
```

If instead the data already is stored as counts, we can create a contingency table by hand by creating a matrix containing the counts:

```{r}
# Input the contingency table directly, if we only have the counts:
lady_results2 <- matrix(c(4, 0, 0, 4),
                       ncol = 2, nrow = 2,
                       byrow = TRUE,
                       dimnames = list(c("Guess: milk first", "Guess: tea first"),
                                       c("Truth: milk first", "Truth: tea first")))
lady_results2
```


To perform Fisher’s exact test, for the lady tasting tea data, we use **fisher.test** as follows.



```{r}

print("The output contains the p-value (0.01429), and some additional information:")
  
fisher.test(lady_results1, alternative = "greater")
fisher.test(lady_results2, alternative = "greater")


```

### One- and two-sided hypotheses

What is that mysterious last argument in fisher.test, alternative = "greater"? Well, there are three different sets of hypotheses that we may want to test:

1) Whether the lady guesses better than random. Here H0: the lady’s guesses are no better than chance, 
                                                 and H1: the lady’s guesses are better than chance.
2) Whether the lady guesses worse than random. Here H0: the lady’s guesses are no worse than chance, 
                                                and H1: the lady’s guesses are worse than chance.
3) Whether the lady’s guesses are either better than or worse than random. Here H0: the lady’s guesses are equally good as chance, 
                                                                            and H1: the lady’s guesses are either worse than or better than chance.

The first two are said to be one-sided or directed, as they specify a direction in which the lady’s ability deviates from chance. The last set of hypotheses is said to be two-sided, because the lady’s abilities can deviate from chance in either direction. Because the alternative hypotheses are different, the three sets of hypotheses will yield different p-values (which is unsurprising, as we are asking different questions depending on how we specify our hypotheses).


```{r}
fisher.test(lady_results2, alternative = "greater") # 1
fisher.test(lady_results2, alternative = "less") # 2
fisher.test(lady_results2, alternative = "two.sided") # 3
```


### The lady binging tea: power and how the sample size affects the analysis

Fisher’s exact test can be used to illustrate an important principle: the greater the sample size,
the easier it is to detect if H1 is true.


...





## 3.4 χ2-tests

When analysing a contingency table containing two variables X and Y, there are two common types of tests. 

The first is a **test of independence**, where we test whether the two variables are independent:

H0: the variables are independent; P(X=x and Y=y)=P(X=x)P(Y=y) for all x,y
H1: the variables are dependent; there is at least one pair x,y such that P(X=x and Y=y)≠P(X=x)P(Y=y).

The second is a **test of homogeneity**, where we test whether the distribution of Y is the same regardless of the value of X, 
which in this case represents different populations:

H0: the variables are independent; P(Y=y|X=x)=P(Y=y) for all x,y
H1: the variables are dependent; there is at least one pair x,y such that P(Y=y|X=x)≠P(Y=y).


Mathematically, a test of homogeneity is equivalent to a test of independence. 
The difference between the two lies in how the data is sampled. When we plan to perform a test of independence,
we sample a single population and then break the observations down into the categories in the contingency table based on their X and Y values. 

When we plan to perform a test of homogeneity we instead sample several populations, 
corresponding to the levels of X, and measure the value of Y.

Finally, in some cases, we have a frequency table showing the distribution of a single variable. 
We may want to **test whether this variable follows some specific distribution, F**, say. 
We then do a goodness-of-fit test of the hypotheses:

H0: the variable follows the distribution F
H1: the variable does not follow the distribution F.

All three of these can be tested using a χ2-test (chi-squared test).

Let’s say that X has cx levels and that Y has cy levels. Then for each of the cx⋅cy cells in the table, we can compare the observed outcome oij to the outcome eij that would be expected if the null hypothesis were true. If the observed outcomes deviate a lot from what we would expect under the null hypothesis, we have evidence against H0

.

Formally, the test is performed by computing the statistic
      
     cx  cy
X2 =  ∑   ∑  (oij−eij)pow2 / eij
     i=1 j=1 

**Large values of this statistic count as evidence against H0**

...

To run the test, we use the **chisq.test** function as follows:

No pipes:
```{r}
chisq.test(ftable(penguins$species, penguins$sex))
```

pipes:
```{r}
penguins |>
  select(species, sex) |>
  ftable() |>
  chisq.test()
```

If we wish to run a goodness-of-fit test, we specify the distribution F given by H0 using the argument p. 
If our null hypothesis is that both sexes are equally common, this would look as follows:

no pipes:
```{r}
chisq.test(ftable(penguins$sex), p = c(0.5, 0.5))
```

pipes:
```{r}
penguins |>
  select(sex) |>
  ftable() |>
  chisq.test(p = c(0.5, 0.5))
```

p-value is high, there is not enough evidence to reject H0 (NOT that it is true)


- When can we use χ2-tests?

χ2-tests are approximate tests that require sufficiently large sample sizes to attain the right type I error rate.

What is “large” is largely determined by how high the smallest expected cell counts are. A common rule-of-thumb is that 
the approximation is adequate as long as all eij>1 and at most 20% of the cells have eij<5 (Cochran, 1954).

To assess this, we can check the expected counts of the cells:

```{r}
# Save the results of the test:
penguins |> 
  select(species, sex) |> 
  ftable() |> 
  chisq.test() -> x2res

# Extract the expected counts:
x2res$expected
```

The chisq.test output will also give a warning if any cells have eij<5.

If your sample size is too small to use a χ2 tests, there are two options in addition to gathering more data. 
The first is to merge some cells by merging levels of one or more of your categorical variables; 
see Section 5.4 for more on how this can be done. 
The second is to use simulated p-values, i.e., to use simulation rather than a mathematical approximation to compute the p-value. 
This turns the test into an approximate permutation test: the X2 statistic is computed for a large number of randomly selected 
permutations of the table, and the p-value is then computed as the proportion of permutations for which X2 was at least 
as large as for the original table. 

To use this approach, we add the argument **simulate.p.value = TRUE** to chisq.test. The argument **B** can also be added, 
to control how many permutations should be simulated; B = 9999 is a reasonable default choice.

No pipes:
```{r}
chisq.test(ftable(penguins$species, penguins$sex), simulate.p.value = TRUE, B = 9999)
```

pipes:
```{r}
penguins |>
  select(species, sex) |>
  ftable() |>
  chisq.test(simulate.p.value = TRUE, B = 9999)
```


**Exercise 3.1**
In cases where our data is a contingency table rather than a data frame with one row for each observation, 
the simplest way to perform a χ2-test is to create a matrix object for the contingency table and run chisq.test with that as the input. 
Here is an example of such data, from a study on antibiotics resistance. In a lab, 18 strains of E.coli bacteria 
and 25 strains of K.pneumoniae bacteria were tested for resistance against an antibiotic. 
The table shows how many strains were resistant:

```{r}
bacteria <- matrix(c(15, 3, 17, 8),
            2, 2,
            byrow = TRUE,
            dimnames = list(c("E.coli", "K.pneumoniae"),
            c("Not resistant", "Resistant")))
bacteria
```

Perform a χ2 test of homogeneity to see if the proportion of resistant strains is the same for both species of bacteria. 
Are the conditions for running a χ2-test met? If not, what can you do instead?


```{r}
chisq.test(bacteria)
```

The p-value is 0.4339, so we have no evidence against H0.

What about the criteria for running the test? We can check the expected counts:

```{r}
chisq.test(bacteria)$expected
```

                Not resistant Resistant
E.coli            13.39535  4.604651
K.pneumoniae      18.60465  6.395349


There is one cell (resistant E.coli) for which the expected count is less than 5. This means that 25 % of the cells have a count below 5, 
and the criteria for running the test are not met. 
We should consider using simulation to compute the p-value, 

```{r}
chisq.test(bacteria, simulate.p.value = TRUE, B = 9999)
```

or using Fisher’s exact test instead:

```{r}
fisher.test(bacteria)
```



## 3.5 Confidence intervals

As a first example, let’s consider confidence intervals for a proportion. We’ll use a function from the MKinfer package. Let’s install it:

```{r}
if (!require(MKinfer)) install.packages('MKinfer', dependencies = T)
library(MKinfer)
```


The **binomCI** function in this package allows us to compute confidence intervals for proportions from binomial experiments using a number of methods. 
The input is the number of “successes” **x**, the sample size **n**, and the **method** to be used.

Let’s say that we want to compute a confidence interval for the proportion of herbivore mammals that sleep for more than 7 hours a day.
First, we need to compute x and n.

No pipes:
```{r}
herbivores <- msleep[msleep$vore == "herbi",]

# Compute the number of animals for which we know the sleep time:
n <- sum(!is.na(herbivores$sleep_total))
cat("Number of herbivores (n):", n, "\n")

# Compute the number of "successes", i.e. the number of animals that sleep
# for more than 7 hours:
x <- sum(herbivores$sleep_total > 7, na.rm = TRUE)
cat("Number of animals that sleep for more than 7 hours (x):", x, "\n")
```

Pipes:
```{r}
msleep |> 
  filter(vore == "herbi") |> 
  select(sleep_total) |> 
  na.omit() |> 
  summarise(n = n(),
            x = sum(sleep_total > 7)) -> res
            
n <- res$n
x <- res$x

cat("Number of herbivores (n):", n, "\n")
cat("Number of animals that sleep for more than 7 hours (x):", x, "\n")
```

The estimated proportion is x/n, which in this case is 0.625. We’d like to quantify the uncertainty in this estimate 
by computing a confidence interval. The standard Wald method, taught in most introductory statistics courses, 
can be computed in the following way:

```{r}
library(MKinfer)

binomCI(x, n, conf.level = 0.95, method = "wald")
```

The confidence interval is printed on the line starting with prob: in this case, it is (0.457,0.792).

Don’t use that method though! The Wald interval is known to be severely flawed (Brown et al., 2001), 
primarily because its coverage can be very far from 1−α. Much better options are available. 
If the proportion can be expected to be close to 0 or 1, the Clopper-Pearson interval is recommended,
and otherwise the Wilson interval is the best choice (Thulin, 2014a).

We can use either of these methods with binomCI:

```{r}
binomCI(x, n, conf.level = 0.95, method = "clopper-pearson")
```


```{r}
binomCI(x, n, conf.level = 0.95, method = "wilson")
```


**Exercise 3.2 **

In a survey, 440 out of 998 randomly sampled respondents said that they plan to vote for a particular candidate in an upcoming election. 
Based on this, compute a 99% Wilson interval for the proportion of voters that plan to vote for this candidate.

```{r}
n <- 998 
x <- 440

binomCI(x, n, conf.level = 0.99, method = "wilson")
```

Solution: (0.400922, 0.4816224)

**Exercise 3.3**

The function binomDiffCI from MKinfer can be used to compute a confidence interval for the difference of two proportions. 
Using the msleep data, use it to compute a confidence interval for the difference between the proportion of herbivores 
that sleep for more than 7 hours a day and the proportion of carnivores that sleep for more than 7 hours a day.


```{r}
carnivores <- msleep[msleep$vore == "carni",]

# Compute the number of animals for which we know the sleep time:
m <- sum(!is.na(carnivores$sleep_total))
cat("Number of carnivores (n):", m, "\n")

# Compute the number of "successes", i.e. the number of animals that sleep
# for more than 7 hours:
y <- sum(carnivores$sleep_total > 7, na.rm = TRUE)
cat("Number of animals that sleep for more than 7 hours (x):", y, "\n")
```

conf. interval:

```{r}
binomDiffCI(x, y, n-x, m-y, level = 0.99, method = "wilson")
```


### Sample size calculations

Confidence intervals that are too wide aren’t that informative. We’d much rather be able to say “the proportion is somewhere
between 0.56 and 0.58” than “the proportion is somewhere between 0.37 and 0.78”.

How wide a confidence interval for a proportion is depends both on the number of successes x and the sample size n.
We don’t know x in advance but can usually control n, at least to some extent. It is therefore often useful to perform
some computations prior to collecting our data, to make sure that n is large enough that we’ll get a confidence interval 
with a reasonable width.

The MKpower package contains several functions that will prove useful when we wish to calculate what sample size we need for a study.

```{r}
# Library libfftw3-dev (needed for Fast Fourier Transforms) is a dependency of qqconf
# To install it:
#
# sudo apt-get update
# sudo apt-get install libfftw3-dev
#  

if (!require(qqconf)) install.packages("qqconf", dependencies = T)
library(qqconf)
if (!require(qqplotr)) install.packages("qqplotr", dependencies = T)
library(qqplotr)

if (!require(MKpower)) install.packages("MKpower", dependencies = T)
library(MKpower)
```

**Exercise 3.4*
What sample size is required to obtain a 95% Wilson confidence interval with expected width 0.05 if the true proportion is 0.7?

Solution: (library cannot be loaded)

```{r}
ssize.propCI(prop = 0.7, width = 0.05,  conf.level = 0.95, method = "wilson")
```

The required sample size is n=1296 (the output says 1295.303, which we round up to 1296).


## 3.6 Comparing mean values

In Section 3.3 we imagined a study where we wanted to find out whether flipper length differs between male and female Chinstrap penguins. 
The penguins dataset can be used for this purpose. It contains measurements of the flipper lengths of a number of penguins. 
If the average length in the female population is μ1 and the average length in the male population is μ2, then the population parameter 
that we are interested in is the difference μ1−μ2

- The hypotheses that we wish to test are:

H0: there is no difference; μ1−μ2=0
H1: there is a difference; μ1−μ2≠0

In this section, we will learn about the t-test, which can be used to test this and similar hypotheses about means and differences of means. 
Throughout the section, we’ll assume that all observations are independent, unless we explicitly say that they aren’t.


### The t-test for comparing two groups

It seems reasonable to base a test of these hypotheses on the difference between the sample means in the two samples: x¯1−x¯2

There is one problem though: said difference is not scale invariant, and so we would get different results depending on whether we measure the flipper lengths in centimetres, metres, or inches. To remedy that, we base our test on the quantity

T = x¯1−x¯2 / s

where s is the standard error of x¯1−x¯2 (i.e., an estimate of the standard deviation of the mean difference). 
How the latter is computed depends on what assumptions we are willing to make. 
The test based on T is called the two-sample t-test. There are two different versions of this test, which differ in how s is computed:

- The equal-variances t-test: where the variance is assumed to be the same in both populations.
- The Welch t-test: where the variance is allowed to differ between the two populations.

The equal-variances t-test will not work properly when the two populations have differing variances
...
The Welch t-test works well both when the population variances differ and when they are equal, and it tests the right hypotheses in both cases. 
It is the default in R, ...

In summary, forget about different versions of the two-sample t-test, and **always use the Welch t-test**. 
As it is the default in R, this is easy to do!

no pipes:
```{r}
library(palmerpenguins)

t.test(flipper_length_mm ~ sex, data = subset(penguins, species == "Chinstrap"))
```

pipes:
```{r}
library(palmerpenguins)

penguins |>
  filter(species == "Chinstrap") |> 
  t.test(flipper_length_mm ~ sex, data = _)  # data = _ means that the data used is the output from the previous step in the pipeline 
```

We are mostly interested in the p-value, which in this case is very low, leading us to reject H0 and conclude that there is a difference.

In addition to the p-value, a 95% confidence interval for the difference μ1−μ2 is also presented 
(remember that there is a close connection between confidence intervals and hypothesis test), along with the means in the two groups.

The confidence interval is (−11.0,−5.3), meaning that the average flipper length in females is somewhere between 5.3 and 11.0 mm shorter than in males.


**Exercise 3.5**
Consider the penguins data again. Run a t-test to see if the average body mass differs between the sexes for Chinstrap penguins.

```{r}
penguins |>
  filter(species == "Chinstrap") |> 
  t.test(body_mass_g ~ sex, data = _) 
```

P-value is very small, so we reject H0 that there is no difference in average weight.
The confidence interval (-569.7903 -253.7391) says that on average female Chinstrap penguins weight between 253 and 560 g less than male ones.

### One-sided hypotheses

The t-test can also be used to test directed hypotheses, such as:

H0: μ1−μ2≥0
H1: μ1−μ2<0

To specify that a one-sided test should be performed, we use the **alternative** argument, 
just as we did for Fisher’s exact test in Section 3.3.4:

no pipes:
```{r}
library(palmerpenguins)

t.test(flipper_length_mm ~ sex,  data = subset(penguins, species == "Chinstrap"), alternative = "less")
```

P-value is very small, so we reject H0 μ1−μ2≥0

pipes:
```{r}
penguins |> 
  filter(species == "Chinstrap")  |> 
  t.test(flipper_length_mm ~ sex,  data = _, alternative = "less")
```

### The t-test for a single sample

In some cases, we wish to test hypotheses about the mean value of a single population. For example:

H0: the mean value equals some specific value μ0; μ=μ0
H1: the mean value does not equal μ0; μ≠μ0


This can be done using a one-sample t-test, which uses a test statistic based on the sample mean ¯x. 
We use t.test and specify the value μ0 of μ under H0 using the argument **mu**. 
Here’s an example where we test whether the average length of the flippers of male Chinstrap penguins is 200 mm:

no pipes:
```{r}
library(palmerpenguins)

t.test(flipper_length_mm ~ 1,
       data = subset(penguins, species == "Chinstrap" &  sex == "male"),
       mu = 200)

# Or, in two steps:
#new_data <- penguins[penguins$species == "Chinstrap" &  penguins$sex == "male",]
#t.test(new_data$flipper_length_mm, mu = 200)
```


pipes:
```{r}
library(palmerpenguins)

penguins |>
  filter(species == "Chinstrap", sex == "male") |> 
  t.test(flipper_length_mm ~ 1, data = _, mu = 200)
```

P-value is much bigger than 0.05 so we cannot reject H0. That is, there is no statistical evidence that the
average flipper length isn't 200 mm.


**Exercise 3.6 **
Consider the penguins data again. Run a one-sided test to see if the average flipper length for male Chinstrap penguins is greater than 195.

pipes:
```{r}
library(palmerpenguins)

penguins |>
  filter(species == "Chinstrap", sex == "male") |> 
  t.test(flipper_length_mm ~ 1, data = _, mu = 195, alternative = "great") # alternative = greater => H0 == less or equal
```

The p-value is 1.7e-05 (0.000017), so we reject the null hypothesis that the average flipper length is less than or equal to 195 mm.


### The t-test for paired samples

In some cases, we have paired observations in our data, i.e., observations that have been collected in pairs. 
A common example is when we make measurements on the same subjects before and after a treatment. 
In such situations, we are often interested in the effect Δ of the treatment, where Δ measures how much 
the mean value changes because of the treatment. A common set of hypotheses are:

H0: there is no effect; Δ=0
H1: there is an effect; Δ≠0.

These can be tested using a paired samples t-test, where the two samples (before and after) are compared 
using the information about the pairing. Here is an example with measurements before and after a treatment 
stored in different columns in a table:

```{r}
exdata <- data.frame(before = c(8.5, 4.4, 9.4, 0.0, 2.2, 9.7, 6.2, 8.2),
                     after = c(8.5,  4.8, 10.1,  1.0,  5.6, 11.4, 7.8, 10.2))
```

no pipes:
```{r}
t.test(exdata$after, exdata$before, paired = TRUE)
```

pipes:
```{r}
exdata |>
  t.test(Pair(after, before) ~ 1, data = _)
```

P-value is (a bit) smaller than 0.05, so we can reject the null hypothesis that there is no effect.
(On average, the difference is in the range (0.4518,2.2482)).

We could run a two-sample t-test to compare the two samples, but that would result in a test with lower power, as it ignores some of the information available to use (namely that the observations belong in pairs). A paired samples t-test is always preferable when you have paired data.

###  When can we use the t-test?

The first requirements for the t-test to work is that the observations are independent. 
(The only exception to this is when we have paired samples).

And then there is the matter of the normal distribution.
...
Unfortunately, real data is almost never normally distributed.

If the variable doesn’t follow a normal distribution (you’ll learn more about assessing the normality assumption in Section 7.1.3), 
the tests and confidence intervals may still be valid if you have large sample sizes, or, for the two-sample test, 
if you have balanced sample sizes, i.e., equally many observations in each group. 
How large “large” is depends on the shape of the distribution. The less symmetric it is, the larger the sample size needs to be.

Today we can use simulation to carry out analyses with fewer assumptions. As an added bonus, these simulation techniques 
often happen to result in statistical methods with better performance than Student’s t-test and other similar methods. 
You’ll learn about two simulation-based ways to compute p-values for the t-test next.


### Permutation t-tests

Continuing our example with the Chinstrap penguin data, we note that there are 34 males and 34 females in the sample – 68 animals in total. 
If there are no differences in flipper length between the sexes (i.e., if H0 is true), the sex labels offer no information about how long 
the flippers are. Under the null hypothesis, the assignment of sex labels to different animals is therefore for all intents and purposes random. 
To find the distribution of the test statistic under the null hypothesis, we could look at all possible ways to assign 34 animals the label 
male and 34 animals the label female. That is, look at all permutations of the labels. The probability of a result at least as extreme as 
that obtained in our sample (in the direction of the alternative), i.e., the p-value, would then be the proportion of permutations that yield a 
t-statistic at least as extreme as that in our sample.

This is a permutation version of the t-test. Note that it doesn’t rely on any assumptions about normality.
...
The idea is that we look at a large number of randomly selected permutations, and check for how many of them we obtain a test statistic 
that is more extreme than the sample test statistic. The law of large numbers guarantees that this proportion will converge to 
the permutation test p-value as the number of randomly selected permutations increases.


No pipes:
```{r}
library(palmerpenguins)
library(MKinfer)

perm.t.test(flipper_length_mm ~ sex, data = subset(penguins, species == "Chinstrap"))
```

Pipes:
```{r}
library(palmerpenguins)
library(MKinfer)

penguins |> 
  filter(species == "Chinstrap") |> 
  perm.t.test(flipper_length_mm ~ sex, data = _)
```

Note that two p-values and confidence intervals are presented: one set from the permutations and one from the traditional approach 
– so make sure that you look at the right ones!

So, you say, how many randomly selected permutations do we need to get an accurate approximation of the permutation test p-value? 
My answer is that, by default, perm.t.test uses 9,999 permutations (you can change that number using the argument R), 
which is widely considered to be a reasonable number. If you are running a permutation test with a much more complex 
(and computationally intensive) statistic, you may have to use a lower number, but avoid that if you can.


### Bootstrap t-tests

A popular method for computing p-values and confidence intervals that resembles the permutation approach is the bootstrap. 
Instead of drawing permuted samples, new observations are drawn with replacement from the original sample, and then 
labels are randomly allocated to them. 
That means that each randomly drawn sample will differ not only in the permutation of labels, but also in what observations are included 
– some may appear more than once and some not at all. 
We can then check what proportion of these bootstrap samples that yield a more extreme test statistic than what we observed in our original sample.
In effect, the p-value is computed in the same way as in the traditional t-test, but with the theoretical normal distribution replaced 
with the empirical distribution of the data. 
We can therefore view the bootstrap t-test as a compromise between the traditional t-test and the permutation t-test.

No pipes:
```{r}
library(palmerpenguins)
library(MKinfer)

boot.t.test(flipper_length_mm ~ sex, data = subset(penguins, species == "Chinstrap"))
```

Pipes:
```{r}
library(palmerpenguins)
library(MKinfer)

penguins |> 
  filter(species == "Chinstrap") |> 
  boot.t.test(flipper_length_mm ~ sex, data = _)
```


### Sample size computations for the t-test

In any study, it is important to collect enough data for the inference that we wish to make. 
If we want to use a t-test for a test about a mean or the difference of two means, what constitutes “enough data” 
is usually measured by the power of the test. The sample is large enough when the test achieves high enough power.

If we are comfortable assuming normality (and we may well be, especially as the main goal with sample size computations 
is to get a ballpark figure), we can use **power.t.test** to compute what power our test would achieve under different settings.

For a two-sample test with unequal variances, we can use **power.welch.t.test** from MKpower instead. 

Both functions can be used to either find the sample size required for a certain power,
or to find out what power will be obtained from a given sample size.

power.t.test and power.welch.t.test both use **delta** to denote the mean difference under the alternative hypothesis. 
In addition, we must supply the standard deviation sd of the distribution. 

Here are some examples:


```{r}
library(MKpower)

# A one-sided one-sample test with 80 % power:
power.t.test(power = 0.8, delta = 1, sd = 1, sig.level = 0.05,
             type = "one.sample", alternative = "one.sided")
```


```{r}
# A two-sided two-sample test with sample size n = 25 and equal
# variances:
power.t.test(n = 25, delta = 1, sd = 1, sig.level = 0.05,
             type = "two.sample", alternative = "two.sided")
```


```{r}
# A one-sided two-sample test with 90 % power and equal variances:
power.t.test(power = 0.9, delta = 1, sd = 0.5, sig.level = 0.01,
             type = "two.sample", alternative = "one.sided")
```


```{r}
# A one-sided two-sample test with 90 % power and unequal variances:
power.welch.t.test(power = 0.9, delta = 1, sd1 = 0.5, sd2 = 1,
                   sig.level = 0.01,
                   alternative = "one.sided")
```

You may wonder how to choose delta and sd. If possible, it is good to base these numbers on a pilot study or related previous work. 
If no such data is available, your guess is as good as mine. For delta, some useful terminology comes from medical statistics, 
where the concept of clinical significance is used increasingly often.
Make sure that delta is large enough to be clinically significant, that is, large enough to actually matter in practice.

If we have reason to believe that the data follows a non-normal distribution, another option is to use simulation to compute 
the sample size that will be required. We’ll do just that in Section 7.3.


**Exercise 3.7** 
Return to the one-sided t-test that you performed in Exercise 3.6. Assume that delta is 5 (i.e., that the true mean is 200)
and that the standard deviation is 6. 
How large does the sample size n have to be for the power of the test to be 95% at a 5% significance level? 
What is the power of the test when the sample size is n=34 (which is the case for the penguins data)?

First, we assume that delta is 5 and that the standard deviation is 6, and want to find the n required 
to achieve 95 % power at a 5 % significance level:

```{r}
power.t.test(power = 0.95, delta = 5, sd = 6, sig.level = 0.05,
             type = "one.sample", 
             alternative = "one.sided")
```

We see than n needs to be at least 18 (17.04 rounded up) to achieve the desired power.

The actual sample size for this dataset is n=34. Let’s see what power that gives us:

```{r}
power.t.test(n = 34, delta = 5, sd = 6, sig.level = 0.05,
             type = "one.sample", 
             alternative = "one.sided")
```

The power is 0.999. We’re more or less guaranteed to find statistical evidence that the mean is greater than 195 if the true mean is 200!

