---
title: "02_cornerstones_of_stadistics"
author: "Fran Camacho"
date: "2025-08-06"
output:
  html_document:
    df_print: paged
---

# 02 - Cornerstones of Stadistics


```{r}
#Libraries

#https://tidyverse.r-universe.dev/tidyverse
if (!require(tidyverse)) install.packages('tidyverse', repos = c('https://tidyverse.r-universe.dev', 'https://cloud.r-project.org'))
library(tidyverse)

# Install the packages:
#install.packages(c("gtsummary", "flextable", "ivo.table"))

if (!require(gtsummary)) install.packages('gtsummary', dependencies = T)
library(gtsummary)
if (!require(flextable)) install.packages('flextable', dependencies = T)
library(flextable)
if (!require(ivo.table)) install.packages('ivo.table', dependencies = T)
library(ivo.table)


```

## 3.1 The three cultures

There are three main schools in statistical modelling: 
the frequentist school, 
the Bayesian school, and 
the machine learning school.


## 3.2 Frequencies, proportions, and cross-tables


```{r}
library(palmerpenguins)
?penguins
```


- Frequency tables:
Frequency tables are used to summarise the distribution of a single variable. 

Without pipes:
```{r}
table(penguins$species)
```

With pipes:
```{r}
library(dplyr)
penguins |> select(species) |> table()
```


- Publication-ready tables

The table in the previous section is fine for data exploration but requires some formatting if you wish to put it in a report. 
If instead you want something publication-ready that you can put straight into a report or a presentation, 
I recommend using either the **gtsummary** package or the **ivo.table** package. 

```{r}
# Install the packages:
#install.packages(c("gtsummary", "flextable", "ivo.table"))

if (!require(gtsummary)) install.packages('gtsummary', dependencies = T)

if (!require(flextable)) install.packages('flextable', dependencies = T)
library(flextable)
if (!require(ivo.table)) install.packages('ivo.table', dependencies = T)
library(ivo.table)
```


Without pipes:
```{r}
library(gtsummary)

tbl_summary(penguins[,"species"])
```

With pipes:
```{r}
library(dplyr)
library(gtsummary)

penguins |>
  select(species) |>
  tbl_summary()
```


The **ivo.table** package and the **ivo_table** function offer a similar table constructed in the same way:

Without pipes:
```{r}
library(ivo.table)

ivo_table(penguins[,"species"])
```


With pipes:
```{r}
library(dplyr)
library(ivo.table)

penguins |>
  select(species) |>
  ivo_table()
```

You can modify the settings to change the colours and fonts used, and to show percentages instead of counts:
```{r}
penguins |>
  select(species) |>
  ivo_table(color = "darkred",
            font = "Garamond",
            percent_by = "row")
```

If you prefer, you can also get the table in a long format that is useful when your variable has many different levels:
```{r}
penguins |>
  select(species) |>
  ivo_table(long_table = TRUE)
```

Finally, the table can be exported to Word as follows:
```{r}
library(flextable)

penguins |>
  select(species) |>
  ivo_table() |> 
  save_as_docx(path = "my_table.docx")
```



- **Contingency tables**

Contingency tables, also known as **cross-tabulations** or **cross-tables**, are used to summarise the relationship between two or more variables.
They consist of a table with rows and columns that represent the different values of the variables, and the entries in the table show the number or proportion of times each combination of values occurs in the data. For example, a contingency table for the variables species and island in the penguins data shows the number of individuals of different species at different islands.


Without pipes:
```{r}
ftable(penguins$island, penguins$species)
```

With pipes:
```{r}
penguins |> select(island, species) |> ftable()
```

Again, we can create a nicely formatted publication-ready table. With ivo_table, we follow the same logic as before:
```{r}
library(ivo.table)

penguins |> select(species, island) |>
            ivo_table()
```

To create a contingency table using tbl_summary, we use the following syntax:

Without pipes:
```{r}
library(gtsummary)

tbl_summary(penguins[,c("species","island")], by = species)
#tbl_summary(penguins[,c("species","island")], by = island)
```


With pipes:
```{r}
library(dplyr)
library(gtsummary)

penguins |> select(species, island) |> tbl_summary(by=species)
```


Contingency tables are great for presenting data, and in many cases we can draw conclusions directly from looking at such a table. 
For instance, in the example above, it is clear that Adelie penguins are the only species present at all three islands 
(or at least, the only species sampled at all three islands). In other cases, the results aren’t as clear-cut. 
That’s when statistical hypothesis testing becomes useful ...


- Three-way and four-way tables

Three-way and four-way tables are contingency tables showing the distribution of three and four variables, respectively. 
They are straightforward to create using ftable or ivo_table.

We simply select the variables we wish to include in the table, and use ftable or ivo_table as in previous examples.
Here are some examples using ivo_table:


```{r}
# A three-way table:
library(ivo.table)

penguins |> select(sex, species, island) |>
            ivo_table()
```


```{r}
# Exclude missing values:
penguins |> select(sex, species, island) |>
            ivo_table(exclude_missing = TRUE)
```

```{r}
# A four-way table:
penguins |> select(sex, species, island, year) |>
            ivo_table()
```


You can’t use gtsummary to construct three-way and four-way tables, but you can use it for a similar type of table, presenting several variables at once, stratified by another variable. For instance, we can show the frequencies of islands and sexes stratified by species, as in the example below. This type of table, showing the distributions of different categorical variables, is often referred to as **Table 1** in scientific papers.

Without pipes:
```{r}
library(gtsummary)

tbl_summary(penguins[,c("species","sex","island")], by = species)
```


With pipes:
```{r}
library(dplyr)
library(gtsummary)

penguins |>
  select(species, sex, island) |>
  tbl_summary(by = species)
```



## 3.3 Hypothesis testing and p-values

https://modernstatisticswithr.com/basicstatistics.html#hypotheses


**Statistical hypothesis testing** is used to determine which of two complementary hypotheses is true. 
In statistics, a hypothesis is a statement about a parameter in a population, such as the population mean value. 
The two hypotheses in a hypothesis testing problem are:

- The null hypothesis H0
corresponding to “no effect”, “no difference”, or “no relationship”.

- The alternative hypothesis H1
corresponding to “there is an effect”, “there is a difference”, or “there is a relationship”.

example: Comparing two groups. 

To find out whether flipper length differs between male and female Chinstrap penguins, we measure the flipper lengths 
of a number of penguins. If the average length in the female population is μ1 and the average length in the male 
population is μ2, then the population parameter that we are interested in is the difference μ1−μ2
The hypotheses are:

H0 : there is no difference; μ1−μ2=0
H1 : there is a difference; μ1−μ2≠0.

The purpose of hypothesis testing is to determine which of the two hypotheses to believe in.
Hypothesis testing is often compared to legal trials: the null hypothesis is considered to be “innocent until proven guilty”, 
meaning that we won’t reject it unless there is compelling evidence against it. 
We can therefore think of the null hypothesis as a sort of default – we’ll believe in it until 
we have enough evidence to say with some confidence that it in fact isn’t true.


**- The lady tasting tea**

...

The hypotheses that this experiment was designed to test are:

H0: the lady’s guesses are no better than chance.
H1: the lady’s guesses are better than chance.

The results of the experiment can be presented in a 2×2 contingency table. 
For instance, one possible outcome is that the lady gets both sets of four right, which yields the following table:

	                 Truth: milk first 	Truth: tea first
Guess: milk first     	4 	               0
Guess: tea first 	      0 	               4 

The design of this experiment is such that all row sums and column sums are 4. 
Consequently, if we know the count in the upper left cell we also know the counts in the other cells.
This means that there are 5 possible tables that can result (the count in the upper left cell can be 4, 3, 2, 1, or 0). 
We can compute the probability of obtaining each table under the null hypothesis21, with the following results:

- The count is 4 (4 cups with milk poured first right), with probability 1/70,
- The count is 3 (3 cups with milk poured first right and 1 wrong), with probability 16/70,
- The count is 2 (2 cups with milk poured first right and 2 wrong), with probability 36/70,
- The count is 1 (1 cup with milk poured first right and 3 wrong), with probability 16/70,
- The count is 0 (4 cups with milk poured first wrong), with probability 1/70.

...

Getting 3 cups where the milk poured first right is the second most extreme outcome. 
If H0 is true, this occurs in 16 experiments out of 70. In addition, a more extreme outcome (4 cups right) occurs in 1 experiment out of 70. 
The probability of getting a result that is at least as extreme as this is therefore 16/70+1/70=17/70, and the p-value given this outcome is 17/70 ≈ 0.24.

...

**- How low does the p-value have to be?**
https://modernstatisticswithr.com/basicstatistics.html#how-low-does-the-p-value-have-to-be

We need some way of determining a cut-off for p-values. Let’s call this cut-off α, the significance level of our test. 
It can be any number between 0 and 1. Once we’ve decided on a value for α, we are ready to make a decision regarding which hypothesis to believe in. 
If the p-value is less than α, we reject H0 in favour of H1 and say the result is statistically significant. 
If the p-value is greater than α, we conclude that there isn’t sufficient evidence against H0, so we’ll believe in it for the time being.

How then shall we choose α? There are two types of errors that we can make in hypothesis testing:

- A type I error: falsely rejecting H0 even though H0 is true (a false positive result).
- A type II error: not rejecting H0 even though H1 is true (a false negative result).

Both of these will depend on what significance level we choose for our p-value. 
If we choose a low α, we require more evidence before we reject H0. 
This lowers the risk of a type I error but increases the risk of a type II error. 

The probability of making a type I error is the easiest to control and often also considered to be the most important of the two.

...

In the real world, there are two types of tests:

- Exact tests, for which the probability of committing a type I error is less than or equal to α
- Approximate tests, for which the probability of committing a type II error is approximately equal to α
(but may be greater than α, perhaps substantially so). How close it is to α depends on a number of factors.
(We’ll discuss these for different tests later in this chapter).


**- Fisher’s exact test**

https://modernstatisticswithr.com/basicstatistics.html#fishers-exact-test


To use Fisher’s exact test, we first need some data. 

The contingency table describing the outcome of the experiment can be constructed in different ways in R, depending on whether our data is stored in a long data frame or already available in tabulated form. If it is stored in a data frame, where each row shows the outcome of a repetition, we can get a contingency table as follows:

```{r}
# Compute the contingency table from a data frame:
lady_data <- data.frame(Guess = c("Milk first", "Milk first",
                                  "Tea first", "Tea first",
                                  "Milk first", "Tea first",
                                  "Tea first", "Milk first"),
                        Truth = c("Milk first", "Milk first",
                                  "Tea first", "Tea first",
                                  "Milk first", "Tea first",
                                  "Tea first", "Milk first"))

lady_data |> ftable() -> lady_results1

lady_results1
```

If instead the data already is stored as counts, we can create a contingency table by hand by creating a matrix containing the counts:

```{r}
# Input the contingency table directly, if we only have the counts:
lady_results2 <- matrix(c(4, 0, 0, 4),
                       ncol = 2, nrow = 2,
                       byrow = TRUE,
                       dimnames = list(c("Guess: milk first", "Guess: tea first"),
                                       c("Truth: milk first", "Truth: tea first")))
lady_results2
```


To perform Fisher’s exact test, for the lady tasting tea data, we use **fisher.test** as follows.



```{r}

print("The output contains the p-value (0.01429), and some additional information:")
  
fisher.test(lady_results1, alternative = "greater")
fisher.test(lady_results2, alternative = "greater")


```

**- One- and two-sided hypotheses**

What is that mysterious last argument in fisher.test, alternative = "greater"? Well, there are three different sets of hypotheses that we may want to test:

1) Whether the lady guesses better than random. Here H0: the lady’s guesses are no better than chance, 
                                                 and H1: the lady’s guesses are better than chance.
2) Whether the lady guesses worse than random. Here H0: the lady’s guesses are no worse than chance, 
                                                and H1: the lady’s guesses are worse than chance.
3) Whether the lady’s guesses are either better than or worse than random. Here H0: the lady’s guesses are equally good as chance, 
                                                                            and H1: the lady’s guesses are either worse than or better than chance.

The first two are said to be one-sided or directed, as they specify a direction in which the lady’s ability deviates from chance. The last set of hypotheses is said to be two-sided, because the lady’s abilities can deviate from chance in either direction. Because the alternative hypotheses are different, the three sets of hypotheses will yield different p-values (which is unsurprising, as we are asking different questions depending on how we specify our hypotheses).


```{r}
fisher.test(lady_results2, alternative = "greater") # 1
fisher.test(lady_results2, alternative = "less") # 2
fisher.test(lady_results2, alternative = "two.sided") # 3
```


**- The lady binging tea: power and how the sample size affects the analysis**

Fisher’s exact test can be used to illustrate an important principle: the greater the sample size,
the easier it is to detect if H1 is true.


...





## 3.4 χ2-tests

When analysing a contingency table containing two variables X and Y, there are two common types of tests. 

The first is a **test of independence*, where we test whether the two variables are independent:

H0: the variables are independent; P(X=x and Y=y)=P(X=x)P(Y=y) for all x,y
H1: the variables are dependent; there is at least one pair x,y such that P(X=x and Y=y)≠P(X=x)P(Y=y).

The second is a **test of homogeneity**, where we test whether the distribution of Y is the same regardless of the value of X, 
which in this case represents different populations:

H0: the variables are independent; P(Y=y|X=x)=P(Y=y) for all x,y
H1: the variables are dependent; there is at least one pair x,y such that P(Y=y|X=x)≠P(Y=y).


Mathematically, a test of homogeneity is equivalent to a test of independence. 

The difference between the two lies in how the data is sampled. When we plan to perform a test of independence,
we sample a single population and then break the observations down into the categories in the contingency table based on their X and Y values. 

When we plan to perform a test of homogeneity we instead sample several populations, 
corresponding to the levels of X, and measure the value of Y.

Finally, in some cases, we have a frequency table showing the distribution of a single variable. 
We may want to **test whether this variable follows some specific distribution, F**, say. 
We then do a goodness-of-fit test of the hypotheses:

H0: the variable follows the distribution F
H1: the variable does not follow the distribution F.

All three of these can be tested using a χ2-test (chi-squared test).


...






## 3.5 Confidence intervals

As a first example, let’s consider confidence intervals for a proportion. We’ll use a function from the MKinfer package. Let’s install it:

```{r}
if (!require(MKinfer)) install.packages('MKinfer', dependencies = T)
library(MKinfer)
```


The **binomCI** function in this package allows us to compute confidence intervals for proportions from binomial experiments using a number of methods. 
The input is the number of “successes” **x**, the sample size **n**, and the **method** to be used.

Let’s say that we want to compute a confidence interval for the proportion of herbivore mammals that sleep for more than 7 hours a day.
First, we need to compute x and n.

No pipes:
```{r}
herbivores <- msleep[msleep$vore == "herbi",]

# Compute the number of animals for which we know the sleep time:
n <- sum(!is.na(herbivores$sleep_total))
cat("Number of herbivores (n):", n, "\n")

# Compute the number of "successes", i.e. the number of animals that sleep
# for more than 7 hours:
x <- sum(herbivores$sleep_total > 7, na.rm = TRUE)
cat("Number of animals that sleep for more than 7 hours (x):", x, "\n")
```

Pipes:
```{r}
msleep |> 
  filter(vore == "herbi") |> 
  select(sleep_total) |> 
  na.omit() |> 
  summarise(n = n(),
            x = sum(sleep_total > 7)) -> res
            
n <- res$n
x <- res$x

cat("Number of herbivores (n):", n, "\n")
cat("Number of animals that sleep for more than 7 hours (x):", x, "\n")
```

The estimated proportion is x/n, which in this case is 0.625. We’d like to quantify the uncertainty in this estimate 
by computing a confidence interval. The standard Wald method, taught in most introductory statistics courses, 
can be computed in the following way:

```{r}
library(MKinfer)

binomCI(x, n, conf.level = 0.95, method = "wald")
```

The confidence interval is printed on the line starting with prob: in this case, it is (0.457,0.792).

Don’t use that method though! The Wald interval is known to be severely flawed (Brown et al., 2001), 
primarily because its coverage can be very far from 1−α. Much better options are available. 
If the proportion can be expected to be close to 0 or 1, the Clopper-Pearson interval is recommended,
and otherwise the Wilson interval is the best choice (Thulin, 2014a).

We can use either of these methods with binomCI:

```{r}
binomCI(x, n, conf.level = 0.95, method = "clopper-pearson")
```


```{r}
binomCI(x, n, conf.level = 0.95, method = "wilson")
```


**Exercise 3.2 **

In a survey, 440 out of 998 randomly sampled respondents said that they plan to vote for a particular candidate in an upcoming election. 
Based on this, compute a 99% Wilson interval for the proportion of voters that plan to vote for this candidate.

```{r}
n <- 998 
x <- 440

binomCI(x, n, conf.level = 0.99, method = "wilson")
```

Solution: (0.400922, 0.4816224)

**Exercise 3.3**

The function binomDiffCI from MKinfer can be used to compute a confidence interval for the difference of two proportions. 
Using the msleep data, use it to compute a confidence interval for the difference between the proportion of herbivores 
that sleep for more than 7 hours a day and the proportion of carnivores that sleep for more than 7 hours a day.


```{r}
carnivores <- msleep[msleep$vore == "carni",]

# Compute the number of animals for which we know the sleep time:
m <- sum(!is.na(carnivores$sleep_total))
cat("Number of carnivores (n):", m, "\n")

# Compute the number of "successes", i.e. the number of animals that sleep
# for more than 7 hours:
y <- sum(carnivores$sleep_total > 7, na.rm = TRUE)
cat("Number of animals that sleep for more than 7 hours (x):", y, "\n")
```

conf. interval:

```{r}
binomDiffCI(x, y, n-x, m-y, level = 0.99, method = "wilson")
```


**- Sample size calculations**

Confidence intervals that are too wide aren’t that informative. We’d much rather be able to say “the proportion is somewhere
between 0.56 and 0.58” than “the proportion is somewhere between 0.37 and 0.78”.

How wide a confidence interval for a proportion is depends both on the number of successes x and the sample size n.
We don’t know x in advance but can usually control n, at least to some extent. It is therefore often useful to perform
some computations prior to collecting our data, to make sure that n is large enough that we’ll get a confidence interval 
with a reasonable width.

The MKpower package contains several functions that will prove useful when we wish to calculate what sample size we need for a study.

```{r}
# Library libfftw3-dev (needed for Fast Fourier Transforms) is a dependency of qqconf
# To install it:
#
# sudo apt-get update
# sudo apt-get install libfftw3-dev
#  

if (!require(qqconf)) install.packages("qqconf", dependencies = T)
library(qqconf)
if (!require(qqplotr)) install.packages("qqplotr", dependencies = T)
library(qqplotr)

if (!require(MKpower)) install.packages("MKpower", dependencies = T)
library(MKpower)
```

**Exercise 3.4*
What sample size is required to obtain a 95% Wilson confidence interval with expected width 0.05 if the true proportion is 0.7?

Solution: (library cannot be loaded)

```{r}
ssize.propCI(prop = 0.7, width = 0.05,  conf.level = 0.95, method = "wilson")
```

The required sample size is n=1296 (the output says 1295.303, which we round up to 1296).





