---
title: "04_EDA_and_unsupervised_learning_5"
author: "Fran Camacho"
date: "2025-09-01"
output: word_document
---

# 4 - Exploratory data analysis and unsupervised learning

https://modernstatisticswithr.com/eda.html#clustering

```{r}
#Libraries

#https://tidyverse.r-universe.dev/tidyverse
if (!require(tidyverse)) install.packages('tidyverse', repos = c('https://tidyverse.r-universe.dev', 'https://cloud.r-project.org'))
library(tidyverse)

```

## 4.12 Cluster analysis

Cluster analysis is concerned with grouping observations into groups, clusters, that in some sense are similar.
Numerous methods are available for this task, approaching the problem from different angles. 
Many of these are available in the cluster package, which ships with R.
In this section, we’ll look at a smorgasbord of clustering techniques.


### Hierarchical clustering

As a first example where clustering can be of interest, we’ll consider the **votes.repub data** from cluster. 
It describes the proportion of votes for the Republican candidate in US presidential elections from 1856 to 1976 in 50 different states:

```{r}
library(cluster)
?votes.repub

View(votes.repub)
```


We are interested in finding subgroups – clusters – of states with similar voting patterns.

To find clusters of similar observations (states, in this case), we could start by assigning each observation to its own cluster.
We’d then start with 50 clusters, one for each observation. Next, we could merge the two clusters that are the most similar, yielding 49 clusters,
one of which consisted of two observations and 48 consisting of a single observation. 
We could repeat this process, merging the two most similar clusters in each iteration until only a single cluster was left.
This would give us a hierarchy of clusters, which could be plotted in a tree-like structure, where observations from the same cluster
would be shown one the same branch. 
Like this:

```{r fig.width=12}
clusters_agnes <- agnes(votes.repub)

plot(clusters_agnes, which = 2)
```

This type of plot is known as a **dendrogram**.

We’ve just used **agnes**, a function from cluster that can be used to carry out hierarchical clustering in the manner described above.
There are a couple of things that need to be clarified, though.

First, how do we measure how similar two p-dimensional observations x and y are? agnes provides two measures of distance between points:

- metric = "euclidean" (the default), uses the Euclidean L2 distance: ||x−y||=sqrt(sum((xi−yi)²))

- metric = "manhattan", uses the Manhattan L1 distance: ||x−y||=sum(|xi−yi|) 


Note that neither of these work if you have categorical variables in your data.
If all your variables are binary, i.e., categorical with two values, you can use **mona** instead of agnes for hierarchical clustering.

Second, how do we measure how similar two clusters of observations are? agnes offers a number of options here. Among them are:

- method = "average" (the default), unweighted average linkage, uses the average distance between points from the two clusters,
- method = "single", single linkage, uses the smallest distance between points from the two clusters,
- method = "complete", complete linkage, uses the largest distance between points from the two clusters,
- method = "ward", Ward’s method, uses the within-cluster variance to compare different possible clusterings, 
with the clustering with the lowest within-cluster variance chosen.


Regardless of which of these you use, it is often a good idea to standardise the numeric variables in your dataset so that they all have the same variance.
If you don’t, your distance measure is likely to be dominated by variables with larger variance, while variables with low variances will have little
or no impact on the clustering. 
To standardise your data, you can use scale:

```{r fig.width=12}
# Perform clustering on standardised data:
clusters_agnes <- agnes(scale(votes.repub))

# Plot dendrogram:
plot(clusters_agnes, which = 2)
```

At this point, we’re starting to use several functions one after another, and so this looks like a perfect job for a pipeline. 
To carry out the same analysis using the |> pipe, we write:

```{r fig.width=12}
votes.repub |> scale() |>
               agnes() |> 
               plot(which = 2)
```

We can now try changing the metric and clustering method used as described above. Let’s use the Manhattan distance and complete linkage:

```{r fig.width=12}
votes.repub |> scale() |>
               agnes(metric = "manhattan", method = "complete") |> 
               plot(which = 2)
```

We can change the look of the dendrogram by adding hang = -1, which causes all observations to be placed at the same level:

```{r fig.width=12}
votes.repub |> scale() |>
               agnes(metric = "manhattan", method = "complete") |> 
               plot(which = 2, hang = -1)
```


As an alternative to agnes, we can consider **diana**. agnes is an agglomerative method, which starts with a lot of clusters and then merge them 
step-by-step. diana, in contrast, is a divisive method, which starts with one large cluster and then step-by-step splits it into several smaller clusters.

```{r fig.width=12}
votes.repub |> scale() |>
               diana() |> 
               plot(which = 2)
```

You can change the distance measure used by setting metric in the diana call. Euclidean distance is the default.

To wrap this section up, we’ll look at two packages that are useful for plotting the results of hierarchical clustering: **dendextend** and **factoextra**. 
We installed factoextra in the previous section, but still need to install dendextend:


```{r}
if (!require(factoextra)) install.packages('factoextra', dependencies = T)
library(factoextra)

if (!require(dendextend)) install.packages('dendextend', dependencies = T)
library(dendextend)
```


To compare the dendrograms produced by different methods (or the same method with different settings), in a **tanglegram**, where the dendrograms are plotted against each other,
we can use tanglegram from dendextend:

```{r}
library(dendextend)
# Create clusters using agnes:
votes.repub |> scale() |>
               agnes() -> clusters_agnes
```


```{r}
# Create clusters using diana:
votes.repub |> scale() |>
               diana() -> clusters_diana
```


```{r fig.width=10}
# Compare the results:
tanglegram(as.dendrogram(clusters_agnes), 
           as.dendrogram(clusters_diana))
```

??????????????????????!!!!!!!!!!!!!!!!!

Some clusters are quite similar here, whereas others are very different.

Often, we are interested in finding a comparatively small number of clusters, k. In hierarchical clustering, we can reduce the number of clusters by “cutting”
the dendrogram tree. To do so using the factoextra package, we first use **hcut** to cut the tree into k parts, and then **fviz_dend** to plot the dendrogram, 
with each cluster plotted in a different colour. If, for instance, we want k=5 clusters and want to use agnes with average linkage and Euclidean distance 
for the clustering, we’d do the following:

```{r}
library(factoextra)

votes.repub |> scale() |>
               hcut(k = 5, hc_func = "agnes",
                    hc_method = "average",
                    hc_metric = "euclidean") |> 
               fviz_dend()
```

There is no inherent meaning to the colours – they are simply a way to visually distinguish between clusters.

Hierarchical clustering is especially suitable for data with named observations. For other types of data, other methods may be better.
We will consider some alternatives next.



**Exercise 4.27**
Continue the last example above by changing the clustering method to complete linkage with the Manhattan distance.

- Do any of the five coloured clusters remain the same?

```{r}
library(cluster)
library(factoextra)

votes.repub |> scale() |>
                hcut(k = 5, hc_func = "agnes",
                     hc_method = "complete",
                     hc_metric = "manhattan") |> 
                fviz_dend()
```
Alaska and Vermont are clustered together in both cases. 

To compare both dendograms:

```{r fig.height=10}
votes.repub |> scale() |>
                hcut(k = 5, hc_func = "agnes",
                     hc_method = "average",
                     hc_metric = "euclidean") |> 
                fviz_dend() -> dendro1
votes.repub |> scale() |>
                hcut(k = 5, hc_func = "agnes",
                     hc_method = "complete",
                     hc_metric = "manhattan") |> 
                fviz_dend() -> dendro2


library(patchwork)
dendro1 / dendro2
```


To compare the two dendrograms in a different way, we can use tanglegram. Setting k_labels = 5 and k_branches = 5 gives us 5 coloured clusters:


- How can you produce a tanglegram with five coloured clusters, to better compare the results from the two clusterings?

```{r fig.height=10}
votes.repub |> scale() |>
                hcut(k = 5, hc_func = "agnes",
                     hc_method = "average",
                     hc_metric = "euclidean") -> clust1
votes.repub |> scale() |>
                hcut(k = 5, hc_func = "agnes",
                     hc_method = "complete",
                     hc_metric = "manhattan") -> clust2

library(dendextend)
tanglegram(as.dendrogram(clust1), 
           as.dendrogram(clust2),
           k_labels = 5,
           k_branches = 5)
```


**Exercise 4.28**
The USArrests data contains statistics on violent crime rates in 50 US states. Perform a hierarchical cluster analysis of the data. 
With which states is Maryland clustered?

```{r fig.width=8}
library(cluster)
library(magrittr)

USArrests |> scale() |>
                agnes() |> 
                plot(which = 2)
```

Maryland is clustered with New Mexico, Michigan and Arizona, in that order.






