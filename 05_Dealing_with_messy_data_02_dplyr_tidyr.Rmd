---
title: "05_Dealing_with_messy_data_02_dplyr_tidyr"
author: "Fran Camacho"
date: "2025-09-19"
output: word_document
---


# 5 - Dealing with messy data


## 5.7 Data manipulation with data.table, dplyr, and tidyr

https://modernstatisticswithr.com/messychapter.html#data-manipulation-with-data.table-dplyr-and-tidyr


In the remainder of this chapter, we will use three packages that contain functions for fast and efficient data manipulation: 
**data.table** and the tidyverse packages **dplyr** and **tidyr**.

```{r}
if (!require(data.table)) install.packages('data.table', dependencies = T)
library(data.table)

if (!require(dplyr)) install.packages('dplyr', dependencies = T)
library(dplyr)

if (!require(tidyr)) install.packages('tidyr', dependencies = T)
library(tidyr)

```


There is almost always more than one way to solve a problem in R. We now know how to access vectors and elements in data frames, e.g., 
to compute means. We also know how to modify and add variables to data frames. Indeed, you can do just about anything using the functions in base R. 

...

**data.table** and the **tidyverse** packages offer simpler solutions and speed up the workflow for these types of problems. Both can be used for the same tasks. 
You can learn one of them or both. The syntax used for data.table is often more concise and arguably more consistent than that in dplyr 
(it is in essence an extension of the [i, j] notation that we have already used for data frames).
Second, it is fast and memory-efficient, which makes a huge difference if you are working with big data (you’ll see this for yourself in Section 6.6). 
On the other hand, many people prefer the syntax in **dplyr** and **tidyr**, which lends itself exceptionally well for usage with pipes.

...

In the sections below, we will see how to perform different operations using both data.table and the tidyverse packages. Perhaps you already know which one you want to use (data.table if performance is important to you, dplyr+tidyr if you like to use pipes or will be doing a lot of work with databases). 

...


###  data.table and tidyverse syntax basics

data.table relies heavily on the [i, j] notation that is used for data frames in R. It also adds a third element: **[i, j, by]**. 
Using this, R selects the rows indicated by i, the columns indicated by j, and groups them by by. This makes it easy, e.g., to compute grouped summaries.

With the tidyverse packages, you will instead use specialised functions with names like filter and summarise to perform operations on your data. 
These are typically combined using the pipe operator, |>, which makes the code flow nicely from left to right.

It’s almost time to look at some examples of what this actually looks like in practice. First though, now that you’ve installed data.table and dplyr, 
it’s time to load them (we’ll get to tidyr a little later). 
We’ll also create a data.table version of the airquality data, which we’ll use in the examples below. This is required in order to use data.table syntax, 
as it only works on data.table objects. Luckily, dplyr works perfectly when used on data.table objects, so we can use the same object for the examples for both packages.

```{r}
library(data.table)
library(dplyr)

aq <- as.data.table(airquality)
```


*When importing data from csv files, you can import them as data.table objects instead of data.frame objects by replacing read.csv with* **fread** *from the data.table package*.
The latter function also has the benefit of being substantially faster when importing large (several megabytes) csv files.

Note that, similar to what we saw in Section 5.2.1, variables in imported data frames can have names that would not be allowed in base R, for instance 
including forbidden characters like -. data.table and dplyr allow you to work with such variables by wrapping their names in apostrophes: 
referring to the illegally named variable as illegal-character-name won’t work, but `illegal-character-name` will.


### Modifying a variable

As a first example, let’s consider how to use **data.table** and **dplyr** to modify a variable in a data frame. 
The wind speed in airquality is measured in miles per hour (mph). We can convert that to metres per second (m/s) by multiplying the speed by 0.44704.
Using only base R, we’d do this using 

```{r}
airquality$Wind <- airquality$Wind * 0.44704
```

With data.table we can instead do this using [i, j] notation, and with dplyr we can do it by using a function called mutate (because it “mutates” your data).

Change wind speed to m/s instead of mph:

With data.table:
```{r}
aq[, Wind := Wind * 0.44704]
```

With dplyr:
```{r}
aq |> mutate(Wind =
         Wind * 0.44704) -> aq
```

Note that when using data.table, there is not an explicit assignment. We don’t use <- to assign the new data frame to aq – instead, 
the assignment happens automatically. This means that you have to be a bit careful, so that you don’t inadvertently make changes 
to your data when playing around with it.

In this case, using data.table or dplyr doesn’t make anything easier. Where these packages really shine is when we attempt more 
complicated operations. Before that though, let’s look at a few more simple examples.


### Computing a new variable based on existing variables

What if we wish to create new variables based on other variables in the data frame? For instance, maybe we want to create a dummy 
variable called Hot, containing a logical that describes whether a day was hot (temperature above 90 - TRUE) or not (FALSE). 
That is, we wish to check the condition Temp > 90 for each row and put the resulting logical in the new variable Hot.

Add a dummy variable describing whether it is hotter than 90:


With data.table:
```{r}
aq[, Hot := Temp > 90]
```

With dplyr:
```{r}
aq |> mutate(Hot = Temp > 90) -> aq
```


### Renaming a variable

To change the name of a variable, we can use setnames from data.table or rename from dplyr. Let’s change the name of the variable Hot that we created in the previous section, to HotDay:

With data.table:
```{r}
setnames(aq, "Hot", "HotDay")
```

With dplyr:
```{r}
aq |> rename(HotDay = Hot) -> aq
```


### Removing a variable

Maybe adding Hot to the data frame wasn’t such a great idea after all. How can we remove it?

Removing Hot:

With data.table:
```{r}
aq[, Hot := NULL]
```

With dplyr:
```{r}
aq |> select(-Hot) -> aq
```


If we wish to remove multiple columns at once, the syntax is similar:

Removing multiple columns:

With data.table:
```{r}
aq[, c("Month", "Day") := NULL]
```

With dplyr:
```{r}
aq |> select(-Month, -Day) -> aq
```



**Exercise 5.16**
Load the VAS pain data vas.csv from Exercise 2.30. Then do the following:

We set file_path to the path for vas.csv and read the data as in Exercise 2.30 and convert it to a data.table
(the last step being optional if we’re only using dplyr for this exercise):

```{r}
file_path <- "data/vas.csv"

vas <- read.csv(file_path, sep = ";", dec = ",", skip = 4)
vas <- as.data.table(vas)
```

A better option is to achieve the same result in a single line by using the **fread** function from data.table:

```{r}
vas <- fread(file_path, sep = ";", dec = ",", skip = 4)
```


- Remove the columns X and X.1.

```{r}
vas[, c("X", "X.1") := NULL]
```

```{r}
vas |> select(-X, -X.1) -> vas
```

- Add a dummy variable called highVAS that indicates whether a patient’s VAS is 7 or greater on any given day.

```{r}
vas[, highVAS := VAS >= 7]
```

```{r}
vas |> mutate(highVAS = VAS >= 7) -> vas
```


### Recoding factor levels

Changing the names of factor levels in base R typically relies on using indices of level names, as in Section 5.4.2.
This can be avoided using data.table or the recode function in dplyr. We return to the smoke example from Section 5.4 and put it in a data.table:

```{r}
library(data.table)
library(dplyr)

smoke <- c("Never", "Never", "Heavy", "Never", "Occasionally", "Never", "Never", "Regularly", "Regularly", "No")

smoke2 <- factor(smoke, levels = c("Never", "Occasionally", "Regularly", "Heavy"),
                        ordered = TRUE)

smoke3 <- data.table(smoke2)
smoke3
```

Suppose that we want to change the levels’ names to abbreviated versions: Nvr, Occ, Reg, and Hvy. Here’s how to do this:

With data.table:
```{r}
new_names = c("Nvr", "Occ", "Reg", "Hvy")
smoke3[.(smoke2 = levels(smoke2), to = new_names),
         on = "smoke2",
         smoke2 := i.to]

smoke3[, smoke2 := droplevels(smoke2)]
smoke3
```

With dplyr:
```{r}
# Version 2:
smoke3 |> mutate(smoke2 = recode(smoke2,
                  "Never" = "Nvr",
                  "Occasionally" = "Occ",
                  "Regularly" = "Reg",
                  "Heavy" = "Hvy")) -> smoke3
```


Next, we can combine the Occ, Reg, and Hvy levels into a single level called Yes:

With data.table:
```{r}
smoke3[.(smoke2 = c("Occ", "Reg", "Hvy"), to = "Yes"),
       on = "smoke2",
       smoke2 := i.to]
```

With dplyr:
```{r}
# Version 2:
smoke3 |> mutate(smoke2 = recode(smoke2,
                  "Occ" = "Yes",
                  "Reg" = "Yes",
                  "Hvy" = "Yes")) -> smoke3
```


**Exercise 5.17**
In Exercise 2.27 you learned how to create a factor variable from a numeric variable using cut.
Return to your solution (or the solution at the back of the book) and do the following using data.table and/or dplyr:

We re-use the solution from Exercise 2.27:

```{r}
airquality$TempCat <- cut(airquality$Temp,
                          breaks = c(50, 70, 90, 110))

aq <- data.table(airquality)
```

- Change the category names to Mild, Moderate, and Hot.

```{r}
new_names = c("Mild", "Moderate",
              "Hot")
aq[.(TempCat = levels(TempCat),
         to = new_names),
         on = "TempCat",
         TempCat := i.to]

aq[,TempCat := droplevels(TempCat)]
```

```{r}
aq |> mutate(TempCat = case_match(TempCat,
              "(50,70]" ~ "Mild",
              "(70,90]" ~ "Moderate",
              "(90,110]" ~ "Hot")) -> aq
```


- Combine Moderate and Hot into a single level named Hot.

```{r}
aq[.(TempCat = c("Moderate", "Hot"),
         to = "Hot"),
         on = "TempCat", TempCat := i.to]

aq[, TempCat := droplevels(TempCat)]
```

```{r}
aq |> mutate(TempCat = case_match(TempCat,
            "Mild" ~ "Mild",
            c("Moderate", "Hot") ~ "Hot"))
```


### Grouped summaries

We’ve already seen how we can use aggregate and by to create grouped summaries.
However, in many cases it is as easy or easier to use data.table or dplyr for such summaries.

To begin with, let’s load the packages again (in case you don’t already have them loaded), 
and let’s recreate the aq data.table, which we made a bit of a mess of by removing some important columns in the previous section:

```{r}
library(data.table)
library(dplyr)

aq <- data.table(airquality)

# class(aq)
#[1] "data.table" "data.frame"
```


Now, let’s compute the mean temperature for each month. Both data.table and dplyr will return a data frame with the results.
In the data.table approach, assigning a name to the summary statistic (mean, in this case) is optional, but not in dplyr.



With data.table:
```{r}
aq[, mean(Temp), Month]

# or, to assign a name:
aq[, .(meanTemp = mean(Temp)),  Month]
```


With dplyr:
```{r}
aq |> group_by(Month) |>
       summarise(meanTemp = mean(Temp))
```

To add that new column:

```{r}
aq |> group_by(Month) |>
       mutate(meanTemp = mean(Temp), na.rm = TRUE) -> aq
#to remove it
aq$meanTemp <- NULL
```


You’ll recall that if we apply mean to a vector containing NA values, it will return NA:

With data.table:
```{r}
aq[, mean(Ozone), Month]
```

With dplyr:
```{r}
aq |> group_by(Month) |>
       summarise(meanTemp = mean(Ozone))
```

In order to avoid this, we can pass the argument na.rm = TRUE to mean, just as we would in other contexts.
To compute the mean ozone concentration for each month, ignoring NA values:

```{r}
aq[, .(meanOzone = mean(Ozone, na.rm = TRUE)),  Month]
```


```{r}
aq |> group_by(Month) |>
       summarise(meanOzone = mean(Ozone, na.rm = TRUE))
```

What if we want to compute a grouped summary statistic involving two variables? 
For instance, the correlation between temperature and wind speed for each month?

With data.table:
```{r}
aq[, .(corTempAndWind = cor(Temp, Wind)), Month]
```


With dplyr:
```{r}
aq |> group_by(Month) |>
       summarise(corTempAndWind = cor(Temp, Wind))
```

The syntax for computing multiple grouped statistics is similar. We compute both the mean temperature and the correlation for each month:

With data.table:
```{r}
aq[, .(meanTemp = mean(Temp), cor = cor(Temp, Wind)), Month]
```

With dplyr:
```{r}
aq |> group_by(Month) |>
       summarise(meanTemp = mean(Temp), cor = cor(Temp, Wind))
```


At times, you’ll want to compute summaries for all variables that share some property. As an example, you may want to compute the mean 
of all numeric variables in your data frame. In dplyr there is a convenience function called **across** that can be used for this: 
summarise(across(where(is.numeric), mean)) will compute the mean of all numeric variables.
In data.table, we can instead utilise the **apply** family of functions from base R, that we’ll study in Section 6.5.,
to compute the mean of all numeric variables:

With data.table:
```{r}
aq[, lapply(.SD, mean, na.rm = TRUE), Month, .SDcols = is.numeric]
```


With dplyr:
```{r}
aq |> group_by(Month) |>
       summarise(across( where(is.numeric), mean, na.rm = TRUE))
```

Both packages have special functions for counting the number of observations in groups: 
**.N** for data.table and **n** for dplyr. For instance, we can count the number of days in each month:



With data.table:
```{r}
aq[, .N, Month]
```

With dplyr:
```{r}
aq |> group_by(Month) |>
       summarise(days = n())
```

Similarly, you can count the number of unique values of variables using uniqueN for data.table and n_distinct for dplyr:

With data.table:
```{r}
aq[, uniqueN(Month)]
```

With dplyr:
```{r}
aq |> summarise(months = n_distinct(Month))
```


**Exercise 5.18**
Load the VAS pain data vas.csv from Exercise 2.30. Then do the following using data.table and/or dplyr:

```{r}
file_path <- "data/vas.csv"
vas <- fread(file_path, sep = ";", dec = ",", skip = 4)
```


- Compute the mean VAS for each patient.

```{r}
vas[, mean(VAS, na.rm = TRUE), ID]
```

```{r}
vas |> group_by(ID) |>
       summarise(meanVAS = mean(VAS, na.rm = TRUE))
```

- Compute the lowest and highest VAS recorded for each patient.

```{r}
vas[, .(min = min(VAS, na.rm = TRUE),
        max = max(VAS, na.rm = TRUE)),
        ID]
```

```{r}
vas |> group_by(ID) |>
       summarise(min = min(VAS, na.rm = TRUE),
                 max = max(VAS, na.rm = TRUE))
```


- Compute the number of high-VAS days, defined as days with where the VAS was at least 7, for each patient.

```{r}
vas[, sum(VAS >= 7, na.rm = TRUE), ID]
```
```{r}
vas |> group_by(ID) |>
       summarise(highVASdays = sum(VAS >= 7, na.rm = TRUE))
```




**Exercise 5.19**
We return to the datasauRus package and the datasaurus_dozen dataset from Exercise 2.28. Check its structure and then do the following using data.table and/or dplyr:

```{r}
library(datasauRus)

dd <- as.data.table(datasaurus_dozen)
```

- Compute the mean of x, mean of y, standard deviation of x, standard deviation of y, and correlation between x and y, grouped by dataset. Are there any differences between the 12 datasets?

```{r}
dd[, .(mean_x = mean(x),
        mean_y = mean(y),
        sd_x = sd(x),
        sd_y = sd(y),
        cor = cor(x,y)),
        dataset]
```

```{r}
dd |> group_by(dataset) |>
       summarise(mean_x = mean(x),
        mean_y = mean(y),
        sd_x = sd(x),
        sd_y = sd(y),
        cor = cor(x,y))
```

- Make a scatterplot of x against y for each dataset. Are there any differences between the 12 datasets?

```{r, fig.width=10}
library(ggplot2)

ggplot(datasaurus_dozen, aes(x, y, colour = dataset)) +
    geom_point() +
    facet_wrap(~ dataset, ncol = 3)
```

Clearly, the datasets are very different! This is a great example of how simply computing summary statistics is not enough. 
They tell a part of the story, yes, but only a part.


### Filling in missing values

In some cases, you may want to fill in missing values of a variable with the previous non-missing entry. To see an example of this, 
let’s create a version of aq where the value of Month are missing for some days:

```{r}
aq$Month[c(2:3, 36:39, 70)] <- NA

# Some values of Month are now missing:
head(aq)
```

To fill in the missing values with the last non-missing entry, we can now use **nafill** or **fill** as follows:

With data.table:
```{r}
aq[, Ozone := nafill(Ozone, "locf")]
aq[, Solar.R := nafill(Solar.R, "locf")]
aq[, Month := nafill(Month, "locf")]
head(aq)
```

With tidyr:
```{r}
aq |> fill(Ozone) -> aq
aq |> fill(Solar.R) -> aq
aq |> fill(Month) -> aq
head(aq)
```

To instead fill in the missing values with the next non-missing entry:

With data.table:
```{r}
aq[, Ozone := nafill(Ozone, "nocb")]
aq[, Solar.R := nafill(Solar.R, "nocb")]
aq[, Month := nafill(Month, "nocb")]
head(aq)
```

With tidyr:
```{r}
aq |> fill(Ozone,
        .direction = "up") -> aq
aq |> fill(Solar.R,
        .direction = "up") -> aq
aq |> fill(Month,
        .direction = "up") -> aq
head(aq)
```


**Exercise 5.20**
Load the VAS pain data vas.csv from Exercise 2.30. 
Fill in the missing values in the Visit column with the last non-missing value.

```{r}
library(data.table)

file_path <- "data/vas.csv"
vas <- fread(file_path, sep = ";", dec = ",", skip = 4)
```

```{r}
head(vas)
```

```{r}
vas[, Visit := nafill(Visit, "locf")]
#vas[, Visit := nafill(V4, "locf")]
head(vas)
```

```{r}
vas |> fill(Visit)
```


### Chaining commands together

When working with tidyverse packages, commands are usually chained together using |> pipes. When using data.table, commands are chained by repeated use of [] brackets
on the same line. This is probably best illustrated using an example. Assume again that there are missing values in Month in aq:

```{r}
aq$Month[c(2:3, 36:39, 70)] <- NA
head(aq)
```

To fill in the missing values with the last non-missing entry (Section 5.7.8) and then count the number of days in each month (Section 5.7.7), we can do as follows:

```{r}
aq[, Month := nafill(Month, "locf")][, .N, Month]
```



```{r}
aq |> fill(Month) |> 
       group_by(Month) |>
       summarise(days = n())
```


## Filtering: select rows

You’ll frequently want to filter away some rows from your data. Perhaps you only want to select rows where a variable exceeds some value, 
or you want to exclude rows with NA values. This can be done in several different ways: using row numbers, using conditions, at random, or using regular expressions.
Let’s have a look at them, one by one. We’ll use aq, the data.table version of airquality that we created before, for the examples.

```{r}
library(data.table)
library(dplyr)

aq <- data.table(airquality)
```


###  Filtering using row numbers

If you know the row numbers of the rows that you wish to remove (perhaps you’ve found them using which, as in Section 2.11.3?),
you can use those numbers for filtering. Here are four examples.

To select row 3:

```{r}
aq[3,]
```

```{r}
aq |> slice(3)
```

To select rows 3 to 5:

```{r}
aq[3:5,]
aq[3:5, Wind]
```


```{r}
aq |> slice(3:5,)

aq |> select(Wind) |> slice(3:5,)
```

To select rows 3, 7, and 15:
To select all rows except rows 3, 7, and 15:


```{r}
aq[c(3, 7, 15),]
aq[-c(3, 7, 15),]
```

```{r}
aq |> slice(c(3, 7, 15))
aq |> slice(-c(3, 7, 15))
```

### Filtering using conditions

Filtering is often done using conditions, e.g., to select observations with certain properties. Here are some examples:

To select rows where Temp is greater than 95:

```{r}
aq[Temp>95,]
```

```{r}
aq |> filter(Temp>95)
```

To select rows where Month is 6 (June):

```{r}
aq[Month==6,]
```

```{r}
aq |> filter(Month==6)
```

To select rows where Temp is greater than 90 and Month is 6 (June):

```{r}
aq[Temp>90 & Month==6,]
```

```{r}
aq |> filter(Temp>90, Month==6)  #also ok using &
```

To select rows where Temp is between 80 and 90 (including 80 and 90):

```{r}
aq[Temp %between% c(80, 90),]
aq[Temp>=80 & Temp<=90,]
```


```{r}
aq |> filter(between(Temp,80,90))
aq |> filter(Temp>=80 & Temp<=90)
```


To select the 4 rows with the highest Temp:

```{r}
aq[frankv(-Temp, ties.method = "min") <= 5, ]   # frankv??? What is that?????
```
It returns 7????  -> Because some have the same value (94 and 93)

```{r}
aq |> top_n(5, Temp)
```


To remove duplicate rows:

```{r}
unique(aq)
```
```{r}
aq |> distinct()
```

To remove rows with missing data (NA values) in at least one variable:

```{r}
na.omit(aq)
```

```{r}
#library(tidyr)

aq |> drop_na()
```

To remove rows with missing Ozone values:

```{r}
na.omit(aq, "Ozone")
```

```{r}
#library(tidyr)

aq |> drop_na("Ozone")
```

At times, you want to filter your data based on whether the observations are connected to observations in a different dataset.
Such filters are known as **semijoins** and **antijoins**. They are discussed in **Section 5.12.4**.


### Selecting rows at random

In some situations, for instance when training and evaluating machine learning models, you may wish to draw a random sample from your data. 
This is done using the **sample** (data.table) and **sample_n** (dplyr) functions.

To select five rows at random:

```{r}
aq[sample(.N, 5),]
```

```{r}
aq |> sample_n(5)
```


### Using regular expressions to select rows

In some cases, particularly when working with text data, you’ll want to filter using regular expressions (see Section 5.5.3). 
data.table has a convenience function called **%like%** that can be used to call **grepl** in an alternative (less opaque?) way. 
With dplyr we use **grepl** in the usual fashion. To have some text data to try this out on, we’ll use this data frame, which contains descriptions of some dogs:

```{r}
dogs <- data.table(Name = c("Bianca", "Bella", "Mimmi", "Daisy",
                            "Ernst", "Smulan"),
                 Breed = c("Greyhound", "Greyhound", "Pug", "Poodle",
                           "Bedlington Terrier", "Boxer"),
                 Desc = c("Fast, playful", "Fast, easily worried",
                          "Intense, small, loud",
                          "Majestic, protective, playful",
                          "Playful, relaxed",
                          "Loving, cuddly, playful"))
View(dogs)
```

To select all rows with names beginning with B:

```{r}
dogs[Name %like% "^B",]
#dogs[Name %like% "^B",c(Name, Breed)]
```


```{r}
# or:
dogs[grepl("^B", Name),]
```

```{r}
#dogs |> filter(grepl("B[a-z]",Name))
dogs |> filter(grepl("^B.",Name))
```

To select all rows where Desc includes the word playful:

```{r}
dogs[Desc %like% "[pP]layful",]
```

```{r}
dogs |> filter(grepl("[pP]layful",Desc))
```




**Exercise 5.21**
Download the ucdp-onesided-191.csv data file from the book’s web page. It contains data about international attacks on civilians by governments 
and formally organised armed groups during the period 1989-2018, collected as part of the Uppsala Conflict Data Program (Eck & Hultman, 2007; Petterson et al., 2019). 
Among other things, it contains information about the actor (attacker), fatality rate, and attack location. Load the data and check its structure.

```{r}
library(dplyr)
library(data.table)

file_path <- "data/ucdp-onesided-191.csv"

ucdp <- fread(file_path)

str(ucdp)
```

```{r}
head(ucdp)
```


- Filter the rows so that only conflicts that took place in Colombia are retained. How many different actors were responsible for attacks in Colombia during the period?

```{r}
colombia <- ucdp[ location == "Colombia",]

unique(colombia$actor_name)
```
```{r}
ucdp |> 
  filter(location == "Colombia") -> colombia

unique(colombia$actor_name)
```


- Using the best_fatality_estimate column to estimate fatalities, calculate the number of worldwide fatalities caused by government attacks on civilians during 1989-2018.

```{r}
gov <- ucdp[actor_name %like% "[gG]overnment",]
```


```{r}
ucdp |> 
  filter(grepl("[gG]overnment", actor_name)) -> gov
```

to list the governments involved in attacks on civilians:

```{r}
unique(gov$actor_name)
```

Not the US??? What a s...t archive.

To estimate the number of fatalities cause by these attacks, we sum the fatalities from each attack:

```{r}
sum(gov$best_fatality_estimate)
```



**Exercise 5.22**
Load the oslo-biomarkers.xlsx data from Exercise 5.8. Use data.table and/or dplyr to do the following:

```{r}
library(dplyr)
library(data.table)
library(openxlsx)

file_path <- "data/oslo-biomarkers.xlsx"

oslo <- as.data.table(read.xlsx(file_path))
```

- Select only the measurements from blood samples taken at 12 months.

```{r}
oslo[PatientID.timepoint %like% "months",]
```

```{r}
oslo |> filter(grepl("months", PatientID.timepoint))
```

- Select only the measurements from the patient with ID number 6.

```{r}
oslo[PatientID.timepoint %like% "^6[-]",]
```

```{r}
oslo |> filter(grepl("^6[-]", PatientID.timepoint))
```


## 5.9 Subsetting: select columns

Another common situation is that you want to remove some variables from your data. Perhaps the variables aren’t of interest in a particular analysis
that you’re going to perform, or perhaps you’ve simply imported more variables than you need. As with rows, this can be done using numbers, names, 
or regular expressions. Let’s look at some examples using the aq data:

```{r}
library(data.table)
library(dplyr)

aq <- data.table(airquality)
```


### Selecting a single column

When selecting a single column from a data frame, you sometimes want to extract the column as a vector and sometimes as a single-column data frame 
(for instance, if you are going to pass it to a function that takes a data frame as input).
You should be a bit careful when doing this, to make sure that you get the column in the correct format:


With data.table:

```{r}
# Return a vector:
#aq$Temp <- that is R base
# or
aq[, Temp]
```

```{r}
# Return a data.table:
aq[, "Temp"]
```

With dplyr:

```{r}
# Return a vector:
#aq$Temp
# or
aq |> pull(Temp)
```


```{r}
# Return a tibble:
aq |> select(Temp)
```

### Selecting multiple columns

Selecting multiple columns is more straightforward, as the object that is returned always will be a data frame. Here are some examples.

```{r}
aq[, .(Temp, Month, Day)]
```


```{r}
aq |> select(Temp, Month, Day)
```

To select all columns between Wind and Month:

```{r}
aq[, Wind:Month]
```


```{r}
aq |> select(Wind:Month)
```


To select all columns except Month and Day:

```{r}
aq[, -c("Month", "Day")]
```


```{r}
#aq |> select(-Month, -Day)
aq |> select(-c(Month, Day))
```


To select all numeric variables (which for the aq data is all variables!):

```{r}
aq[, .SD, .SDcols = is.numeric]
```


```{r}
aq |> select_if(is.numeric)
```

To remove columns with missing (NA) values:

```{r}
aq[, .SD, .SDcols = !anyNA]    # <- whattttt ?????
```


```{r}
aq |> select_if(~all(!is.na(.)))
```



### Using regular expressions to select columns

In data.table, using regular expressions to select columns is done using grep.
dplyr differs in that it has several convenience functions for selecting columns, like starts_with, ends_with, contains. 
As an example, we can select variables the name of which contains the letter n:

With data.table:
```{r}
vars <- grepl("n", names(aq))
aq[, ..vars]
```

With dplyr:
```{r}
# contains is a convenience function for checking if a name contains a string:
aq |> select(contains("n"))

# matches can be used with any regular expression:
aq |> select(matches("n"))  
```


### Subsetting using column numbers

It is also possible to subset using column numbers, but you need to be careful if you want to use that approach.
Column numbers can change, for instance if a variable is removed from the data frame. More importantly, however, 
using column numbers can yield different results depending on what type of data table you’re using.
Let’s have a look at what happens if we use this approach with different types of data tables:

```{r}
# data.frame:
aq <- as.data.frame(airquality)
str(aq[,2])
```


```{r}
# data.table:
aq <- as.data.table(airquality)
str(aq[,2])
```


```{r}
# tibble:
aq <- as_tibble(airquality)
str(aq[,2])
```

As you can see, aq[, 2] returns a vector, a data table or a tibble, depending on what type of object aq is.
Unfortunately, this approach is used by several R packages and can cause problems because it may return the wrong type of object.

A better approach is to use **aq[[2]]**, which works the same for data frames, data tables, and tibbles, returning a vector:

```{r}
# data.frame:
aq <- as.data.frame(airquality)
str(aq[[2]])
```


```{r}
# data.table:
aq <- as.data.table(airquality)
str(aq[[2]])
```


```{r}
# tibble:
aq <- as_tibble(airquality)
str(aq[[2]])
```


**Exercise 5.23**

Return to the ucdp-onesided-191.csv data from Exercise 5.21.
To have a cleaner and less bloated dataset to work with, it can make sense to remove some columns.
Select only the actor_name, year, best_fatality_estimate and location columns.

```{r}
library(dplyr)
library(data.table)

file_path <- "data/ucdp-onesided-191.csv"
  
ucdp <- fread(file_path)
```


```{r}
ucdp[, .(actor_name, year, best_fatality_estimate, location)]
```

```{r}
ucdp |> select(actor_name, year, best_fatality_estimate, location)
```


## 5.10 Sorting

Sometimes you don’t want to filter rows but rearrange their order according to their values for some variable. 
Similarly, you may want to change the order of the columns in your data. I often do this after merging data 
from different tables (as we’ll do in Section 5.12).
This is often useful for presentation purposes but can at times also aid in analyses.

### Changing the column order

It is straightforward to change column positions using **setcolorder** in data.table and **relocate** in dplyr.

To put Month and Day in the first two columns, without rearranging the other columns:

```{r}
setcolorder(aq, c("Month", "Day"))
```

```{r}
aq |> relocate("Month", "Day")
```

Remember with dyplr we don't modify the dataset:

```{r}
head(aq)
```

```{r}
aq |> relocate("Month", "Day") -> aq  # now yes, it is modified

head(aq)
```

### Changing the row order

In data.table, **order** is used for sorting rows, and in dplyr, **arrange** is used (sometimes in combination with desc). 
The syntax differs depending on whether you wish to sort your rows in ascending or descending order. We will illustrate this using the airquality data.

```{r}
library(data.table)
library(dplyr)

aq <- data.table(airquality)
```

First of all, if you’re just looking to sort a single vector, rather than an entire data frame, the quickest way to do so is to use sort:

```{r}
sort(aq$Wind)
sort(aq$Wind, decreasing = TRUE)

sort(c("C", "B", "A", "D"))
```

If you’re looking to sort an entire data frame by one or more variables, you need to move beyond sort. To sort rows by Wind (ascending order):

```{r}
aq[order(Wind),]
```

```{r}
aq |> arrange(Wind)
```

To sort rows by Wind (descending order):

```{r}
aq[order(-Wind),] # desc. order
```

```{r}
aq |> arrange(-Wind)
# or
aq |> arrange(desc(Wind))
```

To sort rows, first by Temp (ascending order) and then by Wind (descending order):

```{r}
aq[order(Temp, -Wind),]
```


```{r}
aq |> arrange(Temp, desc(Wind))
```


**Exercise 5.24**
Load the oslo-biomarkers.xlsx data from Exercise 5.8. Note that it is not ordered in a natural way. Reorder it by patient ID instead.

```{r}
library(dplyr)
library(data.table)
library(openxlsx)

file_path <- "data/oslo-biomarkers.xlsx"

oslo <- as.data.table(read.xlsx(file_path))
```

First, we split the PatientID.timepoint column:

```{r}
oslo[, c("PatientID", "timepoint") := tstrsplit(PatientID.timepoint, "-",
                 fixed = TRUE)]
```

```{r}
oslo |> separate(PatientID.timepoint, into = c("PatientID", "timepoint"), sep = "-") -> oslo
```

Next, we reformat the patient ID to a numeric and sort the table:

```{r}
oslo[, PatientID := as.numeric(PatientID)] 
oslo[order(PatientID),]
```

```{r}
oslo |> mutate(PatientID = as.numeric(PatientID)) |> 
          arrange(PatientID)
```

Finally, we reformat the data from long to wide, keeping the IL-8 and VEGF-A measurements. 
We store it as oslo2, knowing that we’ll need it again in Exercise 5.26.

```{r}
oslo2 <- dcast(oslo,
          PatientID ~ timepoint,
          value.var = c("IL-8", "VEGF-A"))
```

```{r}
oslo |> pivot_wider(id_cols = PatientID, 
                    names_from = timepoint, 
                    values_from = c("IL-8", "VEGF-A")) -> oslo2
```


## 5.11 Reshaping data

The gapminder dataset from the gapminder package contains information about life expectancy, population size and GDP per capita 
for 142 countries for 12 years from 1952 to 2007. To begin with, let’s have a look at the data:

```{r}
library(gapminder)
?gapminder

View(gapminder)
```


Each row contains data for one country and one year, meaning that the data for each country is spread over 12 rows.
This is known as long data or long format. As another option, we could store it in wide format, where the data is
formatted so that all observations corresponding to a country are stored on the same row:

Country        Continent   lifeExp1952 lifeExp1957 lifeExp1962 ...
Afghanistan    Asia         28.8        30.2        32.0       ...
Albania        Europe       55.2        59.3        64.8       ...

Sometimes, it makes sense to spread an observation over multiple rows (long format), and sometimes it makes more sense
to spread a variable across multiple columns (wide format). Some analyses require long data, whereas others require wide data.
And if you’re unlucky, data will arrive in the wrong format for the analysis you need to do. 
In this section, you’ll learn how to transform your data from long to wide, and back again.
 
 
### From long to wide

When going from a long format to a wide format, you choose columns to group the observations by 
(in the gapminder case: country and maybe also continent), columns to take value names from (lifeExp, pop, and gdpPercap),
and columns to create variable names from (year).

In data.table, the transformation from long to wide is done using the **dcast** function. 
dplyr does not contain functions for such transformations, but its sibling, the tidyverse package **tidyr**, does.

The tidyr function used for long-to-wide formatting is **pivot_wider**. First, we convert the gapminder data frame to a data.table object:


```{r}
library(data.table)
library(tidyr)

gm <- as.data.table(gapminder)

head(gm)
```

To transform the gm data from long to wide and store it as gm_wide:

With data.table:

```{r}
gm_wide <- dcast(gm, country + continent ~ year,
             value.var = c("pop", "lifeExp", "gdpPercap"))

head(gm_wide)
```

With tidyr:

```{r}
gm |> pivot_wider(id_cols = c(country, continent),
                  names_from = year,
                  values_from = c(pop, lifeExp, gdpPercap)) -> gm_wide_2

head(gm_wide_2)
```


### From wide to long

We’ve now seen how to transform the long format gapminder data to the wide format gmw data. But what if we want to go from wide format to long?
Let’s see if we can transform gmw back to the long format.

In data.table, wide-to-long formatting is done using **melt**, and in dplyr it is done using **pivot_longer**.

To transform the gmw data from long to wide:

With data.table:

```{r}
gm_original <- melt(gm_wide, id.vars = c("country", "continent"),
                    measure.vars = 2:37)

gm_original
```

This is not the same as the original gm !!!

gm:           1704 obs. of 6 variables
gm_original:  5112 obs. of 4 variables


With tidyr:

```{r}
gm_wide_2 |> pivot_longer(names(gm_wide_2)[2:37],
                    names_to = "variable",
                    values_to = "value") -> gm_original_2

gm_original_2
```

The resulting data frames are perhaps too long, with each variable (pop, lifeExp, and gdpPercapita) being put on a different row.
To make it look like the original dataset, we must first split the variable 'variable' (into a column with variable names and column with years) 
and then make the data frame a little wider again. That is the topic of the next section.


### Splitting columns

In the too long gm data that you created at the end of the last section, the observations in the variable column look like pop_1952 and gdpPercap_2007, i.e., 
are of the form variableName_year. We’d like to split them into two columns: one with variable names and one with years. 
dplyr has a function called **tstrsplit** for this purpose, and **tidyr** has separate.

To split the variable column at the underscore _, and then reformat gm to look like the original gapminder data:

With data.table:

```{r}
gm[, c("variable", "year") := tstrsplit(variable,
                                        "_", fixed = TRUE)]
gm <- dcast(gm, country + year ~  variable,
             value.var = c("value"))
```

With tidyr:

```{r}
gm |> separate(variable,
                into = c("variable", "year"),
                sep = "_") |> 
       pivot_wider(id_cols = c(country, continent, year),
                   names_from = variable,
                   values_from = value) -> gm

gm
```

what ... ???



### Merging columns

Similarly, you may at times want to merge two columns, for instance if one contains the day+month part of a date and the other contains the year.
An example of such a situation can be found in the airquality dataset, where we may want to merge the Day and Month columns into a new Date column.
Let’s re-create the aq data.table object one last time:

```{r}
library(data.table)
library(tidyr)
library(dplyr)

aq <- as.data.table(airquality)
```

If we wanted to create a Date column containing the year (1973), month and day for each observation, we could use paste and as.Date:

```{r}
as.Date(paste(1973, aq$Month, aq$Day, sep = "-"))
```

The natural data.table approach is just this, whereas tidyr offers a function called **unite** to merge columns, which can be combined with mutate 
to paste the year to the date. To merge the Month and Day columns with a year and convert it to a Date object:

With data.table:

```{r}
aq[, Date := as.Date(paste(1973, aq$Month, aq$Day, sep = "-"))]
```

With tidyr and dplyr:

```{r}
aq |> unite("Date", Month, Day, sep = "-") |> mutate(Date = as.Date(paste(1973, Date, sep = "-")))
```



**Exercise 5.25**

Load the oslo-biomarkers.xlsx data from Exercise 5.8. Then do the following using data.table and/or dplyr/tidyr:

```{r}
library(dplyr)
library(tidyr)
library(data.table)
library(openxlsx)


file_path <- "data/oslo-biomarkers.xlsx"

oslo <- as.data.table(read.xlsx(file_path))

```


- Split the PatientID.timepoint column in two parts: one with the patient ID and one with the timepoint.

```{r}
oslo[, c("PatientID","timepoint") := tstrsplit(PatientID.timepoint, "-", fixed = TRUE)]
```

```{r}
oslo |> separate(PatientID.timepoint,
                 into = c("PatientID", "timepoint"), sep = "-") -> oslo
```


- Sort the table by patient ID, in numeric order.

```{r}
oslo[, PatientID := as.numeric(PatientID)] # convert to numeric

oslo[order(PatientID),]  # order by that field
```


```{r}
oslo |> mutate(PatientID = as.numeric(PatientID)) |> 
          arrange(PatientID)
```


- Reformat the data from long to wide, keeping only the IL-8 and VEGF-A measurements.

```{r}
oslo2 <- dcast(oslo,
          PatientID ~ timepoint,
          value.var = c("IL-8", "VEGF-A"))
```

```{r}
oslo |> pivot_wider(id_cols = PatientID, names_from = timepoint,
           values_from = c("IL-8", "VEGF-A")) -> oslo2
```


We save the resulting data frame – we will need it again in Exercise 5.26.



## 5.12 Merging data from multiple tables

It is common that data is spread over multiple tables: different sheets in Excel files, different .csv files, 
or different tables in databases. Consequently, it is important to be able to merge data from different tables.

As a first example, let’s study the sales datasets available from the book’s web page: sales-rev.csv and sales-weather.csv. 
The first dataset describes the daily revenue for a business in the first quarter of 2020, and the second describes
the weather in the region (somewhere in Sweden) during the same period40. 
Store their respective paths as file_path1 and file_path2 and then load them:


```{r}
file_path1 <- "data/sales-rev.csv"
file_path2 <- "data/sales-weather.csv"  

rev_data <- read.csv(file_path1, sep = ";")
weather_data <- read.csv(file_path2, sep = ";")
```


```{r}
str(rev_data)
View(rev_data)
```


```{r}
str(weather_data)
View(weather_data)

```


Next, we wish to subtract three subsets: the revenue in January (rev_jan), the revenue in February (rev_feb),
and the weather in January (weather_jan).

With data.table:

```{r}
rev_jan <- rev_data[rev_data$DATE %between% c("2020-01-01", "2020-01-31"),]
rev_feb <- rev_data[rev_data$DATE %between% c("2020-02-01", "2020-02-29"),]

weather_jan <- weather_data[weather_data$DATE %between% c("2020-01-01", "2020-01-31"),]
```

With dplyr:

```{r}
rev_data |>
  filter(between(as.Date(rev_data$DATE), as.Date("2020-01-01"), as.Date("2020-01-31"))) -> rev_jan

rev_data |> 
  filter(between(as.Date(rev_data$DATE), as.Date("2020-02-01"), as.Date("2020-02-29"))) -> rev_feb

weather_data |> 
  filter(between(as.Date(weather_data$DATE), as.Date("2020-01-01"), as.Date("2020-01-31"))) -> weather_jan
```


A quick look at the structure of the data reveals some similarities:

```{r}
str(rev_jan)
```


```{r}
str(rev_feb)
```


```{r}
str(weather_jan)
```


The rows in rev_jan correspond one-to-one to the rows in weather_jan, with both tables being sorted in exactly the same way.
We could therefore bind their columns, i.e., add the columns of weather_jan to rev_jan.

rev_jan and rev_feb contain the same columns. We could therefore bind their rows, i.e., add the rows of rev_feb to rev_jan. To perform these operations, we can use either base R or dplyr:

With base R:

```{r}
# Join columns of datasets that have the same rows:
cbind(rev_jan, weather_jan)

# Join rows of datasets that have the same columns:
rbind(rev_jan, rev_feb)
```


With dplyr:

```{r}
# Join columns of datasets that have the same rows:
bind_cols(rev_jan, weather_jan)

# Join rows of datasets that have the same columns:
bind_rows(rev_jan, rev_feb)
```



### Merging tables using keys

A closer look at the business revenue data reveals that rev_data contains observations from 90 days,
whereas weather_data only contains data for 87 days; revenue data for 2020-03-01 is missing, and weather data 
for 2020-02-05, 2020-02-06, 2020-03-10, and 2020-03-29 are missing.

Suppose that we want to study how weather affects the revenue of the business. In order to do so, 
we must merge the two tables. We cannot use a simple column bind, because the two tables have different numbers of rows. 
If we attempt a bind, R will produce a merged table by recycling the first few rows from rev_data – note that
the two DATE columns aren’t properly aligned:


```{r}
tail(cbind(rev_data, weather_data))
```

Clearly, this is not the desired output! We need a way to connect the rows in rev_data with the right rows in weather_data.
Put differently, we need something that allows us to connect the observations in one table to those in another.
Variables used to connect tables are known as keys, and must in some way uniquely identify observations. 
In this case the DATE column gives us the key – each observation is uniquely determined by its DATE.
So to combine the two tables, we can combine rows from rev_data with the rows from weather_data that have the same DATE values.
In the following sections, we’ll look at different ways of merging tables using data.table and dplyr.

But first, a word of warning: finding the right keys for merging tables is not always straightforward.
For a more complex example, consider the nycflights13 package, which contains five separate but connected datasets:

```{r}
library(nycflights13)

?airlines  # Names and carrier codes of airlines.
?airports  # Information about airports.
?flights   # Departure and arrival times and delay information for
           # flights.
?planes    # Information about planes.
?weather   # Hourly meteorological data for airports.
```


Perhaps you want to include weather information with the flight data, to study how weather affects delays.
Or perhaps you wish to include information about the longitude and latitude of airports (from airports) in the weather dataset. 
In airports, each observation can be uniquely identified in three different ways: either by its airport code faa, 
its name name, or its latitude and longitude, lat and lon:

```{r}
?airports

head(airports)
```

If we want to use either of these options as a key when merging with airports data with another table, 
that table should also contain the same key.

The weather data requires no less than four variables to identify each observation: origin, month, day and hour:

```{r}
?weather

head(weather)
```

### Inner and outer joins

An operation that combines columns from two tables is called a join. There are two main types of joins: inner joins and outer joins.

- **Inner joins**: create a table containing all observations for which the key appeared in both tables.
So if we perform an inner join on the rev_data and weather_data tables using DATE as the key, it won’t contain data for the days
that are missing from either the revenue table or the weather table.

In contrast, outer joins create a table retaining rows, even if there is no match in the other table. There are three types of outer joins:

- **Left join**: retains all rows from the first table. In the revenue example, this means all dates present in rev_data.
- **Right join**: retains all rows from the second table. In the revenue example, this means all dates present in weather_data.
. **Full join**: retains all rows present in at least one of the tables. In the revenue example, this means all dates present in at least one of rev_data and weather_data.

We will use the rev_data and weather_data datasets to exemplify the different types of joins.
To begin with, we convert them to data.table objects (which is optional if you wish to use dplyr):

```{r}
library(data.table)
library(dplyr)

rev_data <- as.data.table(rev_data)
weather_data <- as.data.table(weather_data)
```


Remember that revenue data for 2020-03-01 is missing, and weather data for 2020-02-05, 2020-02-06, 2020-03-10, and 2020-03-29 are missing. 
This means that out of the 91 days in the period, only 86 have complete data. If we perform an inner join, the resulting table
should therefore have 86 rows.

To perform an inner join of rev_data and weather_data using DATE as key:

With data.table:

```{r}
merge(rev_data, weather_data, by = "DATE")

# Or:
#setkey(rev_data, DATE)
#rev_data[weather_data, nomatch = 0]
```

With dplyr:

```{r}
rev_data |> inner_join(weather_data,  by = "DATE")
```

A left join will retain the 90 dates present in rev_data. To perform a(n outer) left join of rev_data and weather_data using DATE as key:

```{r}
merge(rev_data, weather_data, all.x = TRUE, by = "DATE")

# Or:
#setkey(weather_data, DATE)
#weather_data[rev_data]
```


```{r}
rev_data |> left_join(weather_data, by = "DATE")
```

A right join will retain the 87 dates present in weather_data. To perform a(n outer) right join of rev_data and weather_data using DATE as key:

```{r}
merge(rev_data, weather_data, all.y = TRUE, by = "DATE")

# Or:
#setkey(rev_data, DATE)
#rev_data[weather_data]
```

```{r}
rev_data |> right_join(weather_data, by = "DATE")
```

A full join will retain the 91 dates present in at least one of rev_data and weather_data. To perform a(n outer) full join of rev_data and weather_data using DATE as key:

```{r}
merge(rev_data, weather_data, all = TRUE, by = "DATE")
```


```{r}
rev_data |> full_join(weather_data, by = "DATE")
```


### Semijoins and antijoins

Semijoins and antijoins are similar to joins but work on observations rather than variables. That is, they are used
for filtering one table using data from another table:

- Semijoin: retains all observations in the first table that have a match in the second table.

- Antijoin: retains all observations in the first table that do not have a match in the second table.

The same thing can be achieved using the filtering techniques of Section 5.8, but semijoins and antijoins are simpler
to use when the filtering relies on conditions from another table.

Suppose that we are interested in the revenue of our business for days in February with subzero temperatures. First, we can create a table called filter_data listing all such days:

With data.table:

```{r}
rev_data$DATE <- as.Date(rev_data$DATE)
weather_data$DATE <- as.Date(weather_data$DATE)

filter_data <- weather_data[TEMPERATURE < 0 & DATE %between% c("2020-02-01","2020-02-29"),]
```

With dplyr:

```{r}
rev_data$DATE <- as.Date(rev_data$DATE)
weather_data$DATE <- as.Date(weather_data$DATE)

weather_data |> filter(TEMPERATURE < 0, between(DATE, as.Date("2020-02-01"), as.Date("2020-02-29"))) -> filter_data
```

Next, we can use a semijoin to extract the rows of rev_data corresponding to the days of filter_data:

```{r}
setkey(rev_data, DATE)

rev_data[rev_data[filter_data, which = TRUE]]
```


```{r}
rev_data |> semi_join(filter_data, by = "DATE")
```


If instead we wanted to find all days except the days in February with subzero temperatures, we could perform an antijoin:

```{r}
setkey(rev_data, DATE)
rev_data[!filter_data]
```


```{r}
rev_data |> anti_join(filter_data, by = "DATE")
```



**Exercise 5.26**
We return to the oslo-biomarkers.xlsx data from Exercises 5.8 and 5.25. 
Load the data frame that you created in Exercise 5.25 (or copy the code from its solution). 
You should also load the oslo-covariates.xlsx data from the book’s web page; it contains information about the patients, 
such as age, gender, and smoking habits.

```{r}
library(dplyr)
library(data.table)
library(openxlsx)

file_path_oslo <- "data/oslo-biomarkers.xlsx"
file_path_covar <- "data/oslo-covariates.xlsx"
  
olso <- as.data.table(read.xlsx(file_path_oslo))
covar <- as.data.table(read.xlsx(file_path_covar))
```


Then do the following using data.table and/or dplyr/tidyr:

- Merge the wide data frame from Exercise 5.25 with the oslo-covariates.xlsx data, using patient ID as key.

First, we merge the wide data frame from Exercise 5.25 with the oslo-covariates.xlsx data, using patient ID as key.
A left join, where we only keep data for patients with biomarker measurements, seems appropriate here.
We see that both datasets have a column named PatientID, which we can use as our key.
```{r}
str(oslo2)
```

```{r}
oslo2$PatientID <- as.integer(oslo2$PatientID)
```

```{r}
oslo2 |> left_join(covar, by = "PatientID")
```

- Use the oslo-covariates.xlsx data to select data for smokers from the wide data frame from Exercise 5.25.

Next, we use the oslo-covariates.xlsx data to select data for smokers from the wide data frame using a semijoin. The Smoker.(1=yes,.2=no) column contains information about smoking habits. First we create a table for filtering:

```{r}
covar |>  filter(`Smoker.(1=yes,.2=no)`== 1) -> filter_data
```

Next, we perform the semijoin:

```{r}
oslo2 |> semi_join(filter_data, by = "PatientID")
```


## 5.13 Scraping data from websites

https://modernstatisticswithr.com/messychapter.html#scraping-data-from-websites


## 5.14 Other commons tasks

### Deleting variables

```{r}
rm(my_variable)

# Uncomment to run:
# rm(list = ls())
```

### Importing data from other statistical packages

The foreign library contains functions for importing data from other statistical packages, such as Stata (read.dta),
Minitab (read.mtp), SPSS (read.spss), and SAS (XPORT files, read.xport). 
They work just like read.csv (see Section 2.15), with additional arguments specific to the file format used for
the statistical package in question.

###  Importing data from databases

R and RStudio have excellent support for connecting to databases. However, this requires some knowledge about databases
and topics like ODBC drivers and is therefore beyond the scope of the book.
More information about using databases with R can be found at

https://db.rstudio.com/


###  Importing data from JSON files

JSON is a common file format for transmitting data between different systems. It is often used in web server systems
where users can request data. One example of this is found in the JSON file at:

https://opendata-download-metobs.smhi.se/api/version/1.0/parameter/2/station/98230/period/latest-months/data.json

It contains daily mean temperatures from Stockholm, Sweden, during the last few months, accessible from the Swedish Meteorological
and Hydrological Institute’s server. Have a look at it in your web browser, and then install the **jsonlite** package:

```{r}
if (!require(jsonlite)) install.packages('jsonlite', dependencies = T)
library(jsonlite)
```

We’ll use the fromJSON function from jsonlite to import the data:

```{r}
url <- paste("https://opendata-download-metobs.smhi.se/api/version/",
             "1.0/parameter/2/station/98230/period/latest-months/",
             "data.json",
             sep = "")

stockholm <- fromJSON(url)
stockholm
```

By design, JSON files contain lists, and so stockholm is a list object. The temperature data that we were looking for
is (in this particular case) contained in the list element called value:

```{r}
stockholm$value
```




